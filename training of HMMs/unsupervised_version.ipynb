{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training HMMs when hidden states are truly unobserved\n",
    "\n",
    "In this case we use a expectation maximization (EM) algorithm. The EM algorithm is useful whenever we have to marginalize over an unknown (hidden) variable. Our likelihood is $p(x) = \\sum_{z} p(x, z)$.\n",
    "\n",
    "The E step of the EM algorithm implies computing the analogue of the counting in the case of observed $z$ s. The thing is that, now, these countings depend on $A, B, \\pi, \\alpha$ and $\\beta$ themselves (so we actually need the forward-backward algorithms at this point). The fact that we cannot count directly is caused by the unobserved nature of the $z$ s.\n",
    "\n",
    "E step:\n",
    "$$\n",
    "\\xi_t(i,j) = \\frac{\\alpha_t(i)A_{i,j}B_{j,x_{t+1}}\\beta_{t+1}(j)}{\\sum_{i=1}^M\\sum_{j=1}^M\\alpha_t(i)A_{i,j}B_{j,x_{t+1}}\\beta_{t+1}(j)}\\,,\\\\\n",
    "\n",
    "\\gamma_t(i) = \\sum_{j=1}^M \\xi_t(i,j)\n",
    "$$\n",
    "\n",
    "The M step now computes $\\pi, A$ and $B$ from the quantities $\\xi$ and $\\gamma$\n",
    "\n",
    "M step:\n",
    "\n",
    "$$\n",
    "\\pi_i = \\gamma_1(i)\\,,\\\\\n",
    "A_{i,j} = \\frac{\\sum_{t=1}^{T-1}\\xi_t(i,j)}{\\sum_{t=1}^{T-1}\\gamma_t(i)}\\,,\\\\\n",
    "B_{j,k} = \\frac{\\sum_{t=1}^{T}\\gamma_t(j)\\mathbb{1}(x_t=k)}{\\sum_{t=1}^{T}\\gamma_t(j)}\\,,\n",
    "$$\n",
    "\n",
    "This is an iterative algorithm that starts from guessed values for $A$, $B$ and $\\pi$ and iterates until convergence. The EM algorithm applied to HMMs (the above algorithm) is actually called `Baum-Welch` algorithm. It ensures only convergence to a *local* maximum, and so one could perform more than one training (starting from different initial points in parameter space) and keep the best final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the number of hidden states\n",
    "\n",
    "When states $z$ are really unobserved, we do not know how many hidden states to choose ($M$). This needs to be treated as a hyperparameter of the model. For example, we can choose the $M$ that maximizes the log-lokelihood of the resulting model.\n",
    "\n",
    "Another possibility is to use AIC or BIC, that choose the best model penalizing for the number of parameters (on the training set only!)\n",
    "\n",
    "$$ AIC = 2p- 2 \\log L\\,,\\\\\n",
    "BIC = p\\log N -2\\log L\\,,\n",
    "$$\n",
    "where $p$ is the number of parameters. We can choose the minimum (best) AIC or BIC after evaluating on different $M$s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baum-Welch algorithm for multiple observations\n",
    "\n",
    "We index each observation (each sequence) in our training set by $n$, with $n=1,..., N$. Now we will have $N$ training sequences and, for each one, we will be able to compute a $\\alpha_n\\,, \\beta_n$. There will be also a overall probability of the sequence $P_n$ and a length that of course depends on the sequence (called $T_n$).\n",
    "\n",
    "With this we can use the generalization of our previous formulas (now explicit in $\\alpha, \\beta$) as:\n",
    "$$\n",
    "\\pi_i = \\frac{1}{N}\\sum_{n=1}^N \\frac{\\alpha_n(1,i)\\beta_n(1,i)}{P_n}\\,,\\\\\n",
    "A_{ij} = \\frac{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n-1}\\alpha_n(t,i)A_{ij}B_{j, x_n(t+1)}\\beta_n(t+1,j)}{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n-1}\\alpha_n(t,i)\\beta_n(t,i)}\\,,\\\\\n",
    "B_{jk} = \\frac{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n}\\alpha_n(t,j)\\beta_n(t,j) \\mathbb{1}(x_n(t)=k)}{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n}\\alpha_n(t,j)\\beta_n(t,j)}\\,,\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class HMM that fits the parameters and makes an inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a valid random markov matrix\n",
    "def random_normalized(dim1, dim2):\n",
    "    \"\"\"\n",
    "    Creates a matrix of random numbers that are compatible with Markov\n",
    "    (namely, they rows sum up to 1)\n",
    "    \"\"\"\n",
    "    random_matrix = np.random.rand(dim1, dim2)\n",
    "    #divide by sum over rows to make rows add up to 1 (Markov matrix)\n",
    "    return random_matrix/random_matrix.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HMM\n",
    "class HMM:\n",
    "    def __init__(self, num_hidden_states):\n",
    "        self.M = num_hidden_states\n",
    "\n",
    "    def fit(self, X: np.ndarray, max_iter: int = 30, verbose: bool=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            max_iter: maximum number of EM iterations to allow\n",
    "            X: stores a set of observed sequences as rows.\n",
    "        \"\"\"\n",
    "        # include seed for reproducibility\n",
    "        np.random.seed(123)\n",
    "\n",
    "        # vocabulary size (classes from 0 to V-1)\n",
    "        # (number of x states = K is the maximum number \n",
    "        # of states in the training set found for a sequence)\n",
    "        vocab_size = max(max(x) for x in X) + 1 \n",
    "        num_sequences = len(X)\n",
    "\n",
    "        # initialize pi, A and B\n",
    "        self.pi = np.ones(self.M)/self.M # uniform distirbution for pi\n",
    "        self.A = random_normalized(self.M, self.M)\n",
    "        self.B = random_normalized(self.M, vocab_size)\n",
    "        # store cost\n",
    "        costs = list()\n",
    "        for it in range(max_iter):\n",
    "            #if it % 10 == 0:\n",
    "            #    print('it:', it)\n",
    "            alphas = list()\n",
    "            betas = list()\n",
    "            P = np.zeros(num_sequences) # probabilities\n",
    "            # loop through observations\n",
    "            for n in range(num_sequences):\n",
    "                x = X[n] # n-th sequence\n",
    "                T = len(x) # T_n\n",
    "\n",
    "                #### Compute alpha[t]: forward algorithm\n",
    "                alpha = np.zeros((T, self.M))\n",
    "                alpha[0] = self.pi * self.B[:, x[0]]\n",
    "                for t in range(1, T):\n",
    "                    # * is the element by element multiplication\n",
    "                    alpha[t] = alpha[t-1].dot(self.A) * self.B[:, x[t]]\n",
    "                P[n] = alpha[-1].sum() # probability of a sequence\n",
    "                alphas.append(alpha)\n",
    "\n",
    "                #### Compute beta[t]: backward algorithm\n",
    "                beta = np.zeros((T, self.M))\n",
    "                beta[-1] = 1\n",
    "                for t in range(T-2,-1,-1): # go backwards\n",
    "                    beta[t] = self.A.dot(self.B[:, x[t+1]] * beta[t+1])\n",
    "                betas.append(beta)\n",
    "\n",
    "            #compute cost\n",
    "            cost = np.sum(np.log(P))\n",
    "            costs.append(cost)\n",
    "\n",
    "            #### Reestimate pi, A and B\n",
    "            self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(num_sequences))/num_sequences\n",
    "\n",
    "            denominator_1 = np.zeros((self.M,1))\n",
    "            denominator_2 = np.zeros((self.M,1))\n",
    "            a_num = 0\n",
    "            b_num = 0\n",
    "            for n in range(num_sequences):\n",
    "                x = X[n] # sequence\n",
    "                T = len(x) # T_n \n",
    "\n",
    "                denominator_1 += (alphas[n][:-1] * betas[n][:-1]).sum(axis=0, keepdims=True).T/P[n]\n",
    "                denominator_2 += (alphas[n] * betas[n]).sum(axis=0, keepdims=True).T/P[n]\n",
    "\n",
    "                # nth update for A numerator\n",
    "                a_num_n = np.zeros((self.M, self.M))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(self.M):\n",
    "                        for t in range(T-1):\n",
    "                            a_num_n[i,j] += alphas[n][t,i]*self.A[i,j]*self.B[j,x[t+1]] * betas[n][t+1,j]\n",
    "                a_num += a_num_n/P[n]\n",
    "\n",
    "                # nth update for B numerator\n",
    "                b_num_n = np.zeros((self.M, vocab_size))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(vocab_size):\n",
    "                        for t in range(T):\n",
    "                            if x[t] == j:\n",
    "                                b_num_n[i,j] += alphas[n][t,i] * betas[n][t,i]\n",
    "                b_num += b_num_n/P[n]\n",
    "\n",
    "            # update A and B\n",
    "            self.A = a_num/denominator_1\n",
    "            self.B = b_num/denominator_2\n",
    "\n",
    "            #self.alphas = alphas\n",
    "            #self.betas = betas\n",
    "            ## print & plot final estimates/costs\n",
    "            if verbose:\n",
    "                print('ITERATION:', it)\n",
    "                print(\"A:\", self.A)\n",
    "                print('check A:', self.A.sum(axis=1))\n",
    "                print(\"B:\", self.B)\n",
    "                print('check B:', self.B.sum(axis=1))\n",
    "                print(\"pi:\", self.pi)\n",
    "                print('check pi:', self.pi.sum())\n",
    "        self.alphas = alphas\n",
    "        self.betas = betas\n",
    "\n",
    "        # plot costs\n",
    "        #plt.plot(costs)\n",
    "        #plt.show()\n",
    "\n",
    "    def likelihood(self, x):\n",
    "        '''\n",
    "        Computes the probability (likelihood) of a sequence\n",
    "        by means of the forwards algorithm\n",
    "        '''\n",
    "        T = len(x) # T_n\n",
    "\n",
    "        #### Compute alpha[t]: forward algorithm\n",
    "        alpha = np.zeros((T, self.M))\n",
    "        alpha[0] = self.pi * self.B[:, x[0]]\n",
    "        for t in range(1, T):\n",
    "            # * is the element by element multiplication\n",
    "            alpha[t] = alpha[t-1].dot(self.A) * self.B[:, x[t]]\n",
    "        return alpha[-1].sum()\n",
    "\n",
    "    def likelihood_multi(self, X):\n",
    "        '''\n",
    "        Computes the probability (likelihood) for all the\n",
    "        observations (sequences)\n",
    "        '''\n",
    "        return np.array([self.likelihood(x) for x in X])\n",
    "\n",
    "    def log_likelihood_multi(self, X):\n",
    "        '''\n",
    "        Computes the log likelihood of all the observations\n",
    "        '''\n",
    "        return np.log(self.likelihood_multi(X))\n",
    "\n",
    "    def get_state_sequence(self, x):\n",
    "        '''\n",
    "        Computes the most probable set of hidden states given \n",
    "        observation sequence x using the Viterbi algorithm\n",
    "        '''\n",
    "        T = len(x)\n",
    "        delta = np.zeros((T, self.M))\n",
    "        psi = np.zeros((T, self.M))\n",
    "        delta[0] = self.pi * self.B[:, x[0]]\n",
    "        for t in range(1,T):\n",
    "            for j in range(self.M):\n",
    "                delta[t,j] = np.max(delta[t-1]*self.A[:,j])*self.B[j, x[t]]\n",
    "                psi[t,j] = np.argmax(delta[t-1]*self.A[:,j])\n",
    "\n",
    "        ### Backtrack\n",
    "        states = np.zeros(T, dtype=np.int32)\n",
    "        states[T-1] = np.argmax(delta[T-1])\n",
    "        for t in range(T-2,-1,-1):\n",
    "            states[t] = psi[t+1, states[t+1]]\n",
    "\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the HMM in a dataset of sequences of coin flips with heads (`H`) or tails (`T`) generated with the following parameters:\n",
    "\n",
    "$$\n",
    "\\pi = (0.5,0.5).T\\\\\n",
    "A = \\begin{pmatrix}\n",
    "0.1 & 0.9 \\\\\n",
    "0.8 & 0.2 \n",
    "\\end{pmatrix}\\\\\n",
    "\n",
    "B = \\begin{pmatrix}\n",
    "0.6 & 0.4\\\\\n",
    "0.3 & 0.7\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The file is called `coin_data.txt` and can be found in \n",
    "\n",
    "https://github.com/lazyprogrammer/machine_learning_examples/blob/master/hmm_class/coin_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_coin():\n",
    "    '''\n",
    "    Create training set from coin_data.txt file\n",
    "    and fit a HMM with it\n",
    "    '''\n",
    "    X = list()\n",
    "    for line in open('coin_data.txt'):\n",
    "        x = [1 if elm == 'H' else 0 for elm in line.rstrip()]\n",
    "        X.append(x)\n",
    "\n",
    "    hmm = HMM(2)\n",
    "    hmm.fit(X,max_iter=100)\n",
    "    L = hmm.log_likelihood_multi(X).sum()\n",
    "\n",
    "    print(\"log likelihood after fitting:\", L)\n",
    "    \n",
    "    # set HMM to the actual values that generated the series\n",
    "    # of coin flips\n",
    "    hmm.pi = np.array([0.5, 0.5])\n",
    "    hmm.A = np.array([[0.1, 0.9], [0.8, 0.2]])\n",
    "    hmm.B = np.array([[0.6, 0.4], [0.3, 0.7]])\n",
    "    L = hmm.log_likelihood_multi(X).sum()\n",
    "    print(\"log likelihood of real generating model:\", L)\n",
    "\n",
    "    # try Viterbi on training set\n",
    "    print(\"best state sequence for \", X[0])\n",
    "    print(hmm.get_state_sequence(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial values: [0.5 0.5] [[0.7087962  0.2912038 ]\n",
      " [0.29152056 0.70847944]] [[0.62969057 0.37030943]\n",
      " [0.58883752 0.41116248]]\n",
      "it: 0\n",
      "it: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1644/389656226.py:60: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(num_sequences))/num_sequences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 20\n",
      "it: 30\n",
      "it: 40\n",
      "it: 50\n",
      "it: 60\n",
      "it: 70\n",
      "it: 80\n",
      "it: 90\n",
      "ITERATION: 99\n",
      "A: [[0.70371831 0.29628169]\n",
      " [0.28697428 0.71302572]]\n",
      "check A: [1. 1.]\n",
      "B: [[0.54110179 0.45889821]\n",
      " [0.54024348 0.45975652]]\n",
      "check B: [1. 1.]\n",
      "pi: [0.51003807 0.48996193]\n",
      "check pi: 1.0000000000000002\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood after fitting: -1034.7539242965554\n",
      "log likelihood of real generating model: -1059.7229160265022\n",
      "best state sequence for  [0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "fit_coin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the above algorithms is *underflow*: after all, $\\alpha$ and $\\beta$ are small numbers ($<1$) and hence, for long sequences, the training algorithm will lead to zero for $\\alpha$ really soon, meaning that all multiplications will be zero from that point on. To solve this issue, we will do the following:\n",
    "\n",
    "- For the Viterbi algorithm we will use logs, since it only involve multiplications. Hence, we turn the multiplications into sums and avoid the underflow problem.\n",
    "\n",
    "- For the Baum-Welch algorithm, we will use scaling. For this, we will use a scale factor $c_t\\,, t=1,...,T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorization\n",
    "\n",
    "Trying to run the forward and backward algorithms in general for an arbitrary sequence of length 30 (such as the one here) leads to an out-of-memory situation. \n",
    "We will therefore substitute the value of the sequence at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a valid random markov matrix\n",
    "def random_normalized(dim1, dim2):\n",
    "    \"\"\"\n",
    "    Creates a matrix of random numbers that are compatible with Markov\n",
    "    (namely, they rows sum up to 1)\n",
    "    \"\"\"\n",
    "    random_matrix = np.random.rand(dim1, dim2)\n",
    "    #divide by sum over rows to make rows add up to 1 (Markov matrix)\n",
    "    return random_matrix/random_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "def load_data(data_path='./coin_data.txt'):\n",
    "    X = list()\n",
    "    for line in open(data_path):\n",
    "        x = [1 if elm == 'H' else 0 for elm in line.rstrip()]\n",
    "        X.append(x)\n",
    "    return np.array(X)\n",
    "\n",
    "def generate_initial_parameters(vocab_size,\n",
    "                                num_hidden_states,\n",
    "                                random_seed=True):\n",
    "    if random_seed:\n",
    "        np.random.seed(123)\n",
    "\n",
    "    # initialize pi, A and B\n",
    "    pi = (np.ones(num_hidden_states)/num_hidden_states).reshape(num_hidden_states,1) # uniform distirbution for pi\n",
    "    A = random_normalized(num_hidden_states,num_hidden_states)\n",
    "    B = random_normalized(num_hidden_states, vocab_size)\n",
    "\n",
    "    return pi, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 30)\n"
     ]
    }
   ],
   "source": [
    "X_data = load_data()\n",
    "print(X_data.shape) # 50 samples of 30 timesteps each\n",
    "VOCAB_SIZE = max(max(x) for x in X_data) + 1 # 2 different observable states\n",
    "NUM_SEQS, SEQ_LENGTH = X_data.shape\n",
    "M_ = 2 # assume 2-dim hidden space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward algotihm for fixed input sequence\n",
    "\n",
    "Note for example that $$ \\alpha(t=1, j) = \\pi_j B(j, x_1)\\,,$$ where $x_1$ is just the observation associated to the first timestep. We will substitute explicitly this observation. Now, our dataset does not contain only one observation, but $N = 50$ of them. To vectorize everything, we will substitute the $N$ samples of $x_1$ observed.\n",
    "\n",
    "This logic will be applied to the recursive algorithm as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_initial(input_sequences: np.ndarray, \n",
    "                    pi: Union[List, np.ndarray], \n",
    "                    B: Union[List, np.ndarray]) -> np.ndarray:\n",
    "    '''\n",
    "    Computes the value of alpha associated to initial time t=1\n",
    "    \n",
    "    Parameters\n",
    "        input_sequences: np.ndarray\n",
    "            Input set of sequences, i.e. the training data\n",
    "        pi: Union[List, np.ndarray]\n",
    "            Initial vector of probabilities (i.e. p(z_1)) for each possible state of z_1\n",
    "        B:  Union[List, np.ndarray]\n",
    "            time-independent matrix of transition probabiltiies p(x_t|z_t) for each x- and z-states\n",
    "\n",
    "    Return\n",
    "        np.ndarray: initial value of alpha (namely, alpha_1)\n",
    "    '''\n",
    "    assert np.isclose(np.sum(pi), 1), \"the sum of probabilities of initial latent states must add up to 1\"\n",
    "    assert (np.isclose(B.sum(axis=1), 1)).all(), \"the total probability of a given observed state must be 1\"\n",
    "    # element-wise product with B evaluated on x(t=1) for all the rows (all the samples) at once\n",
    "    # alpha_initial will end up having dimension M x N, where N is the total number of samples\n",
    "    return np.multiply(pi, B[:,X_data[:,0]])\n",
    "\n",
    "def beta_initial(A_dim: int, num_sequences: int) -> np.ndarray:\n",
    "    '''\n",
    "    Computes the value of beta associated to initial time t=T\n",
    "\n",
    "    Parameters\n",
    "        A_dim: int\n",
    "            dimension of matrix A (namely, M in the above notation)\n",
    "\n",
    "    Return\n",
    "        np.ndarray: initial value of beta (namely, beta_1)\n",
    "    '''\n",
    "    return np.ones(shape=(A_dim, num_sequences)) # column vector of ones (one component per each z-state)\n",
    "\n",
    "\n",
    "def forward_algo_input_fixed(\n",
    "                    input_sequences: np.ndarray, \n",
    "                    A: Union[List, np.ndarray], \n",
    "                    B: Union[List, np.ndarray], \n",
    "                    alpha_ini: np.ndarray\n",
    "                    ) -> List[np.ndarray]:\n",
    "\n",
    "    ''' \n",
    "    This is the forward algorithm as implemented in notebook forward_algo.ipynb\n",
    "    but with the input sequences evaluated whenever possible to make the problem\n",
    "    computationally light\n",
    "\n",
    "    Parameters\n",
    "        input_sequences: np.ndarray\n",
    "            Input set of sequences, i.e. the training data\n",
    "        A:  Union[List, np.ndarray]\n",
    "            Initial vector of probabilities (i.e. p(z_1)) for each possible state of z_1\n",
    "        B:  Union[List, np.ndarray]\n",
    "            time-independent matrix of transition probabiltiies p(x_t|z_t) for each x- and z-states\n",
    "        alpha_ini: np,ndarray\n",
    "            The value of alpha(t=1, z_1)\n",
    "    Return\n",
    "        List[np.ndarray]: list of alphas of the form \n",
    "        [alpha(t=1, z_1),..., alpha(t=T-1, z_{T-1}), alpha(t=T, z_T)]\n",
    "    '''\n",
    "    sequence_length = input_sequences.shape[1]\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    assert isinstance(sequence_length, int), 'the length of the sequence must be an integer number'\n",
    "    assert alpha_ini.shape[0] == A.shape[0], 'the length of the first axis of alpha_ini must be the rows of A'\n",
    "\n",
    "    alphas_list = [alpha_ini] \n",
    "    for t_ in range(1,sequence_length):\n",
    "        # np.einsum('ij,il->jl', A, alphas_list[t_-1]) gives the first part of the algorithm\n",
    "        # factor1_jt = sum_i A_ij alpha(i,t) -> shape M x N\n",
    "        # The second part of the formula np.multiply(B[:, input_sequences[:,t_]], factor1)\n",
    "        # gives B(j, x_t) * factor1_jt\n",
    "        alphas_list.append(np.multiply(B[:, input_sequences[:,t_]], \n",
    "                                        np.einsum('ij,il->jl', A, alphas_list[t_-1])))\n",
    "    #alpha list has shape T x M x N\n",
    "    return np.array(alphas_list)\n",
    "\n",
    "def backward_algo_input_fixed(input_sequences: np.ndarray, \n",
    "                    A: Union[List, np.ndarray], \n",
    "                    B: Union[List, np.ndarray], \n",
    "                    beta_ini: np.ndarray\n",
    "                    ) -> List[np.ndarray]:\n",
    "\n",
    "    '''\n",
    "    Computes a set of alphas following the (recursive) forward algorithm\n",
    "\n",
    "    Parameters\n",
    "        input_sequences: np.ndarray\n",
    "            Input set of sequences, i.e. the training data\n",
    "        A:  Union[List, np.ndarray]\n",
    "            Initial vector of probabilities (i.e. p(z_1)) for each possible state of z_1\n",
    "        B:  Union[List, np.ndarray]\n",
    "            time-independent matrix of transition probabiltiies p(x_t|z_t) for each x- and z-states\n",
    "        alpha_ini: np,ndarray\n",
    "            The value of alpha(t=1, z_1)\n",
    "    Return\n",
    "        List[np.ndarray]: list of betas of the form \n",
    "        [beta(t=1, z_1), ..., beta(t=T-1, z_{T-1}), beta(t=T, z_T)]\n",
    "    '''\n",
    "    sequence_length = input_sequences.shape[1]\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    assert isinstance(sequence_length, int), 'the length of the sequence must be an integer number'\n",
    "    assert beta_ini.shape[0] == A.shape[0], 'the length of the first axis of beta_ini must be the rows of A'\n",
    "\n",
    "    # initialize betas list\n",
    "    betas_list = [0]*sequence_length\n",
    "    betas_list[-1] = beta_ini #beta_ini will be a M x N matrix of 1s\n",
    "    for t_ in reversed(range(1,sequence_length)):\n",
    "        # note that we don't use t+1 when indexing beta because \n",
    "        # python starts counting indexes from 0: reversed(range(0,sequence_length))\n",
    "        # goes from sequence_length-1 to 0. The last index of betas_list\n",
    "        # is also sequence_length-1\n",
    "        exps1 = np.einsum('ij,js,jl->isl', A, B[:, input_sequences[:,t_]], betas_list[t_])\n",
    "        # force (column of B) = (column of betas_list[t_]) = x_{t+1}  \n",
    "        # i.e pick diagonal for first and last indices: s = l above, without summing over s!\n",
    "        betas_list[t_-1] = np.diagonal(exps1, axis1=len(exps1.shape)-2, axis2=len(exps1.shape)-1)\n",
    "        # note that index associated to z is first index of beta\n",
    "        # (this is so also for beta_ini)\n",
    "    return np.array(betas_list)\n",
    "\n",
    "def probability_fixed_input_sequence(alpha: np.ndarray):\n",
    "    '''\n",
    "    Computes the probability of a fixed sequence of x states given an alpha \n",
    "    (this alpha is suppossed to correspond to alpha(t=T, z_T))\n",
    "\n",
    "    Parameters:\n",
    "        alpha: np.ndarray\n",
    "            A tensor with dimensions (M, K, K,...,K)\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            A tensor with dimension (K, K,...,K)\n",
    "    '''\n",
    "    # sum alpha along the first direction (that would correspond to z_T)\n",
    "    # returns probability for arbitrary set of x\n",
    "    # (note the ordering: ijk... will correspond to x_T, x_{T-1}, x_{T-2},...)\n",
    "    #return alpha.sum(axis=0)\n",
    "    return np.einsum('i...->...', alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_, A_, B_ = generate_initial_parameters(vocab_size=VOCAB_SIZE, num_hidden_states=M_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Forward algorithm**\n",
    "\n",
    "For a given sequence (given row) \n",
    "\n",
    "$$B(i,x_2) \\rightarrow M \\times 1$$\n",
    "$$ \\alpha(t=1,i) \\rightarrow M \\times 1$$\n",
    "$$ A(j,i) \\rightarrow M \\times M$$\n",
    "Recall that:\n",
    "$$\\alpha(t+1, i) = B(i, x_{t+1}) \\sum_j^M A(j,i) \\alpha(t, j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ini = alpha_initial(X_data, pi_, B_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17681246697497588"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: \n",
    "# third sequence (n=3) only for z=2 (i.e. alpha(t=2, z=2))\n",
    "assd=0\n",
    "for i in range(2):\n",
    "    assd += A_[i,1]*alpha_ini[i,2] #z, t and n start from 0!\n",
    "\n",
    "B_[1, X_data[2,1]]*assd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17681246697497588"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the full list of alphas for all N\n",
    "alpha_list = forward_algo_input_fixed(X_data, A_, B_, alpha_ini)\n",
    "alpha_list[1][1,2] #alpha for t=2, z=2 and n=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert B_[1, X_data[2,1]]*assd == alpha_list[1][1,2] #alpha for t=2, z=2 and n=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (probability_fixed_input_sequence(alpha_list[-1]) > 0).all()\n",
    "assert (probability_fixed_input_sequence(alpha_list[-1]) < 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = probability_fixed_input_sequence(alpha_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Backward algorithm**\n",
    "\n",
    "For a given sequence \n",
    "\n",
    "$$B(i,x_{29}) \\rightarrow M \\times 1$$\n",
    "$$ \\beta(t=T,i) \\rightarrow M \\times 1$$\n",
    "$$ A(j,i) \\rightarrow M \\times M$$\n",
    "Recall that:\n",
    "$$\\beta(t, i) = \\sum_j^M A(i,j)B(j, x_{t+1})\\beta(t+1, j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_ini = beta_initial(M_, NUM_SEQS)\n",
    "beta_ini.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6007470210010455"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z=2, n=3, t=29 from  beta_ini (recall that beta_ini corresponds to t=T=30)\n",
    "basse = 0\n",
    "for j in range(M_):\n",
    "    basse += A_[1,j]*B_[j, X_data[2,-1]]*beta_ini[j, 2]\n",
    "basse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6007470210010455"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_list = backward_algo_input_fixed(X_data, A_, B_, beta_ini)\n",
    "beta_list[-2][1,2] # dimension T x M x N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert basse == beta_list[-2][1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update $\\pi$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pi(alpha_list: np.ndarray, beta_list: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Formula to update \\pi according to Baum Welch\n",
    "\n",
    "    Three equivalent ways\n",
    "\n",
    "    Method 1: \n",
    "        np.multiply(beta_list[0]/prob,alpha_list[0]).mean(axis=1)\n",
    "        \n",
    "    Method 2:\n",
    "        llo = 0\n",
    "        for n in range(NUM_SEQS):\n",
    "            llo += beta_list[0][:,n] * alpha_list[0][:, n]/prob[n]\n",
    "        llo/NUM_SEQS\n",
    "\n",
    "    Method 3:\n",
    "        ss = np.diag(np.einsum('ij,lj', beta_list[0]/prob, alpha_list[0]))\n",
    "        ss/NUM_SEQS\n",
    "\n",
    "    Parameters:\n",
    "        alpha_list: np.ndarray\n",
    "            A tensor with dimensions (T, M, N)\n",
    "        beta_list: np.ndarray\n",
    "            A tensor with dimensions (T, M, N)\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            A tensor with dimension (M, 1)\n",
    "    '''\n",
    "    prob = probability_fixed_input_sequence(alpha_list[-1])\n",
    "    return (np.multiply(beta_list[0]/prob, alpha_list[0]).mean(axis=1)).reshape(beta_list.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial values: [0.5 0.5] [[0.7087962  0.2912038 ]\n",
      " [0.29152056 0.70847944]] [[0.62969057 0.37030943]\n",
      " [0.58883752 0.41116248]]\n",
      "it: 0\n",
      "NUMERATOR [[503.76144211 211.15307487]\n",
      " [211.3125875  523.77289552]]\n",
      "DENOMINATOR [[714.91451698]\n",
      " [735.08548302]]\n",
      "ITERATION: 0\n",
      "A: [[0.7046457  0.2953543 ]\n",
      " [0.28746669 0.71253331]]\n",
      "check A: [1. 1.]\n",
      "B: [[0.56095853 0.43904147]\n",
      " [0.52091592 0.47908408]]\n",
      "check B: [1. 1.]\n",
      "pi: [0.49581661 0.50418339]\n",
      "check pi: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2321/3462682197.py:60: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(num_sequences))/num_sequences\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2321/3172505148.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mB_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2321/3462682197.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, max_iter)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# plot costs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "hmm = HMM(2)\n",
    "hmm.pi = pi_\n",
    "hmm.A = A_\n",
    "hmm.B = B_\n",
    "hmm.fit(X_data, max_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update A\n",
    "\n",
    "$$ \n",
    "\n",
    "A_{ij} = \\frac{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n-1}\\alpha_n(t,i)A_{ij}B_{j, x_n(t+1)}\\beta_n(t+1,j)}{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n-1}\\alpha_n(t,i)\\beta_n(t,i)}\\,,\\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_A(input_sequences: np.ndarray, \n",
    "            alpha_list: np.ndarray, \n",
    "            beta_list: np.ndarray,\n",
    "            A_old: np.ndarray,\n",
    "            B_old: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    prob = probability_fixed_input_sequence(alpha_list[-1])\n",
    "\n",
    "    # be careful with reshape! It does not move axis directly!\n",
    "    # use np.moveaxis to turn shape (a,b,c) into (b,c,a), for example\n",
    "    # turn dimension of B_old[:, input_sequences[:,1:]] into shape of beta_list[1:,:,:]\n",
    "    interm_ = np.moveaxis(B_old[:, input_sequences[:,1:]],-1,0)\n",
    "    # note that in the formula B(j,x_{n,t+1}) involves th eexact same indices as\n",
    "    # beta_n(t,j), so we can multiply element by element\n",
    "    # also divide by P_n straight away (see formula)\n",
    "    beta_B = (interm_*beta_list[1:,:,:]/prob)\n",
    "\n",
    "    # We use the X[1:,:,:] in the indices to denote that what enters is t+1\n",
    "\n",
    "    # A_{ij} does not involve t, n, so we move it away from sums\n",
    "    # sum in t goes from t=1 to T-1. This involves using alpha_list[:-1,:,:]\n",
    "    # (t+1) goes from 1 to T, that explain the indices of B and beta above\n",
    "    # alpha coes with index z=i and beta_B with z=j, that is why those indices\n",
    "    # remain free to match with A_old{ij}\n",
    "    numerator = A_old*np.einsum('ijk,irk->jr', alpha_list[:-1,:,:], beta_B) # dim MxM\n",
    "    '''\n",
    "    Another way:\n",
    "        numerator = np.einsum('ijk,lm,nki,irk->jlmnr', alpha_list[:-1,:,:], A_, B_[:, input_sequences[:,1:]], beta_list[1:,:,:]/prob)\n",
    "        numerator = np.diagonal(numerator, axis1=0, axis2=1)\n",
    "        numerator = np.diagonal(numerator, axis1=0, axis2=1)\n",
    "        numerator = np.diagonal(numerator, axis1=0, axis2=len(numerator.shape)-1)\n",
    "    '''\n",
    "    denominator = np.diag(np.einsum('ikj,isj->ks', alpha_list[:-1,:,:], beta_list[:-1,:,:]/prob)) # dim M \n",
    "\n",
    "    # see formula: we want numerator[i,j]/denominator[i]\n",
    "    #reshape denominator to acquire dimension Mx1\n",
    "    return numerator/denominator.reshape(denominator.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7046457 , 0.2953543 ],\n",
       "       [0.28746669, 0.71253331]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_A(X_data, alpha_list, beta_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update B\n",
    "\n",
    "$$\n",
    "B_{jk} = \\frac{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n}\\alpha_n(t,j)\\beta_n(t,j) \\mathbb{1}(x_n(t)=k)}{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n}\\alpha_n(t,j)\\beta_n(t,j)}\\,,\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_B(input_sequences: np.ndarray, \n",
    "            alpha_list: np.ndarray, \n",
    "            beta_list: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    prob = probability_fixed_input_sequence(alpha_list[-1])\n",
    "\n",
    "    vocab_size = max(max(x) for x in input_sequences) + 1\n",
    "    # define delta-type object, that is only 1 whenever k equals x_n(t)\n",
    "    mask_u = list()\n",
    "    for k_ in range(vocab_size):\n",
    "        # place 1. if condition is fullfilled, 0. otherwise\n",
    "        mask_u.append(np.where(input_sequences == k_, 1., 0.))\n",
    "    mask_u = np.array(mask_u) # dim KxNxT\n",
    "\n",
    "    # indices of alpha_list and beta_list coincide, so we can multiply elemen by element\n",
    "    unmasked_num = (alpha_list*beta_list/prob)\n",
    "    # mask numerator with mask_u\n",
    "    numerator = np.einsum('ijk,lki->jl', unmasked_num, mask_u) # dim MxK\n",
    "    #denominator is just the unmasked numerator summed over t and n\n",
    "    denominator = np.einsum('ijk->j', unmasked_num) # dim M \n",
    "\n",
    "    # see formula: we want numerator[j,k]/denominator[j]\n",
    "    #reshape denominator to acquire dimension Mx1\n",
    "    return numerator/denominator.reshape(denominator.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56095853, 0.43904147],\n",
       "       [0.52091592, 0.47908408]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_B(X_data, alpha_list, beta_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorized Viterbi\n",
    "\n",
    "$$ \\delta(t=1, j) = \\pi(j) B(j,x_1)$$\n",
    "\n",
    "$$ \\delta(t, j) = \\text{max}_{i\\in \\{1,...,M\\}} \\left[\\delta(t-1,i) A(i,j)\\right] B(j, x_t)\\,, \\hspace{1cm} j=1,...,M $$\n",
    "\n",
    "$$p^* = \\text{max}_{i\\in \\{1,...,M\\}} \\delta (t=T, j) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_init = np.multiply(pi_, B_[:, X_data[:,0]])\n",
    "delta_init.shape # M x N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.einsum('ij,kl ->ijkl', A_, delta_init)\n",
    "ssd = np.diagonal(ss, axis1=0, axis2=2) \n",
    "ssdm = np.max(ssd, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(ssd, axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Backtrack\n",
    "states = np.zeros(T, dtype=np.int32)\n",
    "states[T-1] = np.argmax(delta[T-1])\n",
    "for t in range(T-2,-1,-1):\n",
    "    states[t] = psi[t+1, states[t+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ssdm * B_[:, X_data[:, 1]]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13123696 0.05391776]\n",
      "[0.05993116 0.14565008]\n"
     ]
    }
   ],
   "source": [
    "print(ssd[:,1,0])\n",
    "print(ssd[:,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_1 = np.multiply(delta_init, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(input_sequences: np.ndarray, \n",
    "            pi: np.ndarray,\n",
    "            A: np.ndarray,\n",
    "            B: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    sequence_length = input_sequences.shape[1]\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    pi = np.array(pi).reshape(B.shape[0], 1)\n",
    "\n",
    "    # define initial delta\n",
    "    delta_ini = np.multiply(pi, B[:, input_sequences[:, 0]]) # M x N\n",
    "\n",
    "    deltas_list = [delta_ini]\n",
    "    # initial psi = 0 with dim MxN\n",
    "    psis_list = [np.zeros(shape=(B.shape[0], input_sequences.shape[0]))] \n",
    "    for t_ in range(1,sequence_length):\n",
    "        # compute A(i,j)*delta(t-1, i) (no summation!) by first getting all indices\n",
    "        # and then picking the appropriate diagonal (axis=0 (i) with axis=2 (k))\n",
    "        extend_elmnt_prod = np.einsum('ij,kl->ijkl', A_, deltas_list[t_-1])\n",
    "        elmnt_prod = np.diagonal(extend_elmnt_prod, axis1=0, axis2=2) # M(j) x N x M(i)\n",
    "        # get maximum in the direction of i (axis=2)\n",
    "        max_elmnt_prod = np.max(elmnt_prod, axis=2) # M(j) x N\n",
    "        # multiply by B[:, input_sequences[:, t]] (also with dim = M x N)\n",
    "        # and append\n",
    "        deltas_list.append(np.multiply(max_elmnt_prod, \n",
    "                    B[:, input_sequences[:, t_]])) # element appended has dim= M x N\n",
    "        # append psi[t,j]\n",
    "        psis_list.append(np.argmax(elmnt_prod, axis=2)) # M(j) x N\n",
    "    return deltas_list, psis_list\n",
    "    ### Backtracking algorithm\n",
    "    optim_states = np.zeros_like(input_sequences) # N x T\n",
    "    #[np.zeros(shape=(1,input_sequences.shape[0]))]*sequence_length\n",
    "    #optim_states[-1] = np.argmax(deltas_list[-1], axis=0) # argmax(delta(T)) has dim = N\n",
    "    optim_states[:,-1] = np.argmax(deltas_list[-1], axis=0) # argmax(delta(T)) has dim = N\n",
    "    #return np.array(psis_list), optim_states\n",
    "    # go barckards\n",
    "    for t_ in reversed(range(1,sequence_length)):\n",
    "        # preserve time direction\n",
    "        # first compute general tensor with all indices\n",
    "        # and then pick diagonal: so for example, for t_=29\n",
    "        #you get np.array(psis_list)[29, optim_states[i,29], i] for i=0,...,N-1\n",
    "        optim_states[:,t_-1] = np.diagonal(np.array(psis_list)[t_, \n",
    "                                                    optim_states[:, t_], :]) # diag of N x N matrix\n",
    "\n",
    "    return np.array(psis_list), optim_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_ini = np.multiply(pi_, B_[:, X_data[:, 0]]) # M x N\n",
    "deltas_list = [delta_ini]\n",
    "extend_elmnt_prod = np.einsum('ij,kl->ijkl', A_, deltas_list[0])\n",
    "# A(i,j)delta(0,i)\n",
    "elmnt_prod = np.diagonal(extend_elmnt_prod, axis1=0, axis2=2)\n",
    "max_elmnt_prod = np.max(elmnt_prod, axis=2)\n",
    "argmax_elmnt_prod = np.argmax(elmnt_prod, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax_elmnt_prod[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22316114, 0.08582912],\n",
       "       [0.13123696, 0.05993116],\n",
       "       [0.22316114, 0.08582912],\n",
       "       [0.22316114, 0.08582912],\n",
       "       [0.13123696, 0.05993116],\n",
       "       [0.22316114, 0.08582912],\n",
       "       [0.22316114, 0.08582912],\n",
       "       [0.13123696, 0.05993116],\n",
       "       [0.22316114, 0.08582912],\n",
       "       [0.22316114, 0.08582912]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmnt_prod[0,:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.13123696, 0.05993116],\n",
       "        [0.22316114, 0.08582912],\n",
       "        [0.22316114, 0.08582912]],\n",
       "\n",
       "       [[0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.05391776, 0.14565008],\n",
       "        [0.09168414, 0.20858964],\n",
       "        [0.09168414, 0.20858964]]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmnt_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22316114, 0.13123696, 0.22316114, 0.22316114, 0.13123696,\n",
       "        0.22316114, 0.22316114, 0.13123696, 0.22316114, 0.22316114,\n",
       "        0.22316114, 0.13123696, 0.22316114, 0.22316114, 0.22316114,\n",
       "        0.13123696, 0.13123696, 0.22316114, 0.22316114, 0.22316114,\n",
       "        0.13123696, 0.13123696, 0.22316114, 0.13123696, 0.13123696,\n",
       "        0.22316114, 0.13123696, 0.22316114, 0.22316114, 0.13123696,\n",
       "        0.13123696, 0.13123696, 0.22316114, 0.22316114, 0.22316114,\n",
       "        0.22316114, 0.22316114, 0.13123696, 0.13123696, 0.13123696,\n",
       "        0.13123696, 0.22316114, 0.13123696, 0.22316114, 0.13123696,\n",
       "        0.13123696, 0.13123696, 0.13123696, 0.22316114, 0.22316114],\n",
       "       [0.20858964, 0.14565008, 0.20858964, 0.20858964, 0.14565008,\n",
       "        0.20858964, 0.20858964, 0.14565008, 0.20858964, 0.20858964,\n",
       "        0.20858964, 0.14565008, 0.20858964, 0.20858964, 0.20858964,\n",
       "        0.14565008, 0.14565008, 0.20858964, 0.20858964, 0.20858964,\n",
       "        0.14565008, 0.14565008, 0.20858964, 0.14565008, 0.14565008,\n",
       "        0.20858964, 0.14565008, 0.20858964, 0.20858964, 0.14565008,\n",
       "        0.14565008, 0.14565008, 0.20858964, 0.20858964, 0.20858964,\n",
       "        0.20858964, 0.20858964, 0.14565008, 0.14565008, 0.14565008,\n",
       "        0.14565008, 0.20858964, 0.14565008, 0.20858964, 0.14565008,\n",
       "        0.14565008, 0.14565008, 0.14565008, 0.20858964, 0.20858964]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_elmnt_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "psps, opt_st = viterbi(X_data, pi_, A_, B_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]),\n",
       " array([[0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]])]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.28834417e-14, 5.80204314e-15, 6.33459904e-14, 1.65783703e-15,\n",
       "        2.19075750e-14, 2.10595020e-15, 2.19075750e-14, 1.65783703e-15,\n",
       "        1.07716328e-13, 2.81905953e-15, 1.28834417e-14, 1.28834417e-14,\n",
       "        1.28834417e-14, 1.68313051e-15, 7.57651495e-15, 3.72526111e-14,\n",
       "        3.01599134e-15, 7.57651495e-15, 3.72526111e-14, 6.33459904e-14,\n",
       "        1.28834417e-14, 6.33459904e-14, 7.57651495e-15, 2.19075750e-14,\n",
       "        2.19075750e-14, 3.72526111e-14, 1.82921516e-15, 1.07716328e-13,\n",
       "        1.07716328e-13, 7.57651495e-15, 3.72526111e-14, 6.33459904e-14,\n",
       "        1.28834417e-14, 3.72526111e-14, 7.57651495e-15, 1.96844033e-15,\n",
       "        3.72526111e-14, 1.28834417e-14, 6.33459904e-14, 2.17192693e-15,\n",
       "        1.28834417e-14, 1.07716328e-13, 1.28834417e-14, 2.19075750e-14,\n",
       "        6.33459904e-14, 1.28834417e-14, 4.45560900e-15, 4.45560900e-15,\n",
       "        6.33459904e-14, 6.33459904e-14],\n",
       "       [1.88181346e-14, 9.17513760e-15, 5.52742108e-14, 4.47351193e-15,\n",
       "        2.69499873e-14, 4.47351193e-15, 2.69499873e-14, 4.47351193e-15,\n",
       "        7.91597735e-14, 6.40664401e-15, 1.88181346e-14, 1.88181346e-14,\n",
       "        1.88181346e-14, 3.12368051e-15, 1.31399762e-14, 3.85958453e-14,\n",
       "        6.40664401e-15, 1.31399762e-14, 3.85958453e-14, 5.52742108e-14,\n",
       "        1.88181346e-14, 5.52742108e-14, 1.31399762e-14, 2.69499873e-14,\n",
       "        2.69499873e-14, 3.85958453e-14, 4.47351193e-15, 7.91597735e-14,\n",
       "        7.91597735e-14, 1.31399762e-14, 3.85958453e-14, 5.52742108e-14,\n",
       "        1.88181346e-14, 3.85958453e-14, 1.31399762e-14, 4.47351193e-15,\n",
       "        3.85958453e-14, 1.88181346e-14, 5.52742108e-14, 4.47351193e-15,\n",
       "        1.88181346e-14, 7.91597735e-14, 1.88181346e-14, 2.69499873e-14,\n",
       "        5.52742108e-14, 1.88181346e-14, 9.17513760e-15, 9.17513760e-15,\n",
       "        5.52742108e-14, 5.52742108e-14]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psps[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_st[4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll=[]\n",
    "for i in range(50):\n",
    "    ll.append(psps[29,opt_st[i,29],i])\n",
    "\n",
    "len(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diagonal(psps[29,opt_st[:,29],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('ijk,lm->ijklm', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psps[29, : ,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2, 50)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207/3495326239.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  kk = np.array(opt_st)[-1][1]\n"
     ]
    }
   ],
   "source": [
    "psps_s = psps[:,:,1]\n",
    "kk = np.array(opt_st)[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psps_s[29,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207/2409934690.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(opt_st)[-1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(opt_st)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psps[29, :, opt_st[29]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "28\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_207/3995043669.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# preserve time direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mopt_st\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_st\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "for t_ in reversed(range(1,X_data.shape[1])):\n",
    "    print(t_)\n",
    "    # preserve time direction\n",
    "    opt_st[t_-1] =np.array(psps)[t_, opt_st[t_], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2, 50)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psps.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tensorized HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedHMM:\n",
    "\n",
    "    def __init__(self, num_hidden_states):\n",
    "        self.M = num_hidden_states\n",
    "\n",
    "    def fit(self, X: np.ndarray, max_iter: int = 30, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            max_iter: maximum number of EM iterations to allow\n",
    "            X: stores a set of observed sequences as rows.\n",
    "        \"\"\"\n",
    "\n",
    "        # include seed for reproducibility\n",
    "        np.random.seed(123)\n",
    "\n",
    "        # vocabulary size (classes from 0 to V-1)\n",
    "        # (number of x states = K is the maximum number \n",
    "        # of states in the training set found for a sequence)\n",
    "        vocab_size = max(max(x) for x in X) + 1 \n",
    "        num_sequences = len(X)\n",
    "\n",
    "        # initialize pi, A and B\n",
    "        self.pi, self.A, self.B = generate_initial_parameters(\n",
    "                                                vocab_size=vocab_size, \n",
    "                                                num_hidden_states=self.M)\n",
    "        # store cost\n",
    "        costs = list()\n",
    "        for it in range(max_iter):\n",
    "            #if it % 10 == 0:\n",
    "            #    print('it:', it)\n",
    "\n",
    "            #### Compute alpha\n",
    "            alpha_ini = alpha_initial(X, self.pi, self.B)\n",
    "            # compute the full list of alphas for all N\n",
    "            self.alpha_list = forward_algo_input_fixed(X, self.A, self.B, alpha_ini)\n",
    "\n",
    "            #### compute beta\n",
    "            beta_ini = beta_initial(self.M, num_sequences)\n",
    "            self.beta_list = backward_algo_input_fixed(X, self.A, self.B, beta_ini)\n",
    "\n",
    "            #### compute cost\n",
    "            P = probability_fixed_input_sequence(self.alpha_list[-1])\n",
    "            cost = np.sum(np.log(P))\n",
    "            costs.append(cost)\n",
    "\n",
    "            #### Reestimate pi, A and B\n",
    "            self.pi = update_pi(self.alpha_list, self.beta_list)\n",
    "            self.A = update_A(X, self.alpha_list, self.beta_list, self.A, self.B)\n",
    "            self.B = update_B(X, self.alpha_list, self.beta_list)\n",
    "\n",
    "            if verbose:\n",
    "                ## print & plot final estimates/costs\n",
    "                print('ITERATION:', it)\n",
    "                print(\"A:\", self.A)\n",
    "                print('check A:', self.A.sum(axis=1))\n",
    "                print(\"B:\", self.B)\n",
    "                print('check B:', self.B.sum(axis=1))\n",
    "                print(\"pi:\", self.pi)\n",
    "                print('check pi:', self.pi.sum())\n",
    "\n",
    "        \n",
    "        # plot costs\n",
    "        #plt.plot(costs)\n",
    "        #plt.show()\n",
    "\n",
    "    def likelihood(self, x):\n",
    "        '''\n",
    "        Computes the probability (likelihood) of a sequence\n",
    "        by means of the forward algorithm\n",
    "        '''\n",
    "        \n",
    "        alpha_ini = alpha_initial(x, self.pi, self.B)\n",
    "        # compute the full list of alphas for all N\n",
    "        self.alpha_list = forward_algo_input_fixed(x, self.A, self.B, alpha_ini)\n",
    "\n",
    "        return probability_fixed_input_sequence(self.alpha_list[-1])\n",
    "\n",
    "    def likelihood_multi(self, X):\n",
    "        '''\n",
    "        Computes the probability (likelihood) for all the\n",
    "        observations (sequences)\n",
    "        '''\n",
    "        #return np.array([self.likelihood(x) for x in X])\n",
    "        return self.likelihood(X)\n",
    "\n",
    "    def log_likelihood_multi(self, X):\n",
    "        '''\n",
    "        Computes the log likelihood of all the observations\n",
    "        '''\n",
    "        return np.log(self.likelihood_multi(X))\n",
    "\n",
    "    def get_state_sequence(self, x):\n",
    "        # TODO: TENSORIZE\n",
    "        '''\n",
    "        Computes the most probable set of hidden states given \n",
    "        observation sequence x using the Viterbi algorithm\n",
    "        '''\n",
    "        T = len(x)\n",
    "        delta = np.zeros((T, self.M))\n",
    "        psi = np.zeros((T, self.M))\n",
    "        delta[0] = self.pi * self.B[:, x[0]]\n",
    "        for t in range(1,T):\n",
    "            for j in range(self.M):\n",
    "                delta[t,j] = np.max(delta[t-1]*self.A[:,j])*self.B[j, x[t]]\n",
    "                psi[t,j] = np.argmax(delta[t-1]*self.A[:,j])\n",
    "\n",
    "        ### Backtrack\n",
    "        states = np.zeros(T, dtype=np.int32)\n",
    "        states[T-1] = np.argmax(delta[T-1])\n",
    "        for t in range(T-2,-1,-1):\n",
    "            states[t] = psi[t+1, states[t+1]]\n",
    "\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# the random state is fixed so that the initial state is identical.\n",
    "thmm = TensorizedHMM(2)\n",
    "hmm = HMM(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_time_stats = dict()\n",
    "for m_iter in [10,30,50]:\n",
    "    tensor_time_stats[m_iter] = list()\n",
    "    for exp_ in range(300):\n",
    "        #if exp_ % 10 == 0:\n",
    "        #    print(exp_)\n",
    "        start = time.time()\n",
    "        thmm = TensorizedHMM(2)\n",
    "        thmm.fit(X_data, max_iter=m_iter)\n",
    "        tensor_time_stats[m_iter].append(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2321/3611249952.py:59: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(num_sequences))/num_sequences\n"
     ]
    }
   ],
   "source": [
    "basic_time_stats = dict()\n",
    "for m_iter in [10,30,50]:\n",
    "    basic_time_stats[m_iter] = list()\n",
    "    for exp_ in range(300):\n",
    "        #if exp_ % 10 == 0:\n",
    "        #    print(exp_)\n",
    "        start = time.time()\n",
    "        hmm = HMM(2)\n",
    "        hmm.fit(X_data, max_iter=m_iter)\n",
    "        basic_time_stats[m_iter].append(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(tensor_time_stats[30])\n",
    "plt.hist(basic_time_stats[30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(tensor_time_stats[50])\n",
    "plt.hist(basic_time_stats[50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(tensor_time_stats[10])\n",
    "plt.hist(basic_time_stats[10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04507279396057129\n",
      "0.27582740783691406\n"
     ]
    }
   ],
   "source": [
    "print(np.max(tensor_time_stats))\n",
    "print(np.max(basic_time_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19f2dcfaf593399b30a8f53af9c897705b95d8f5016d0f862fa64694b4ff6a68"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('HMMs_udemy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

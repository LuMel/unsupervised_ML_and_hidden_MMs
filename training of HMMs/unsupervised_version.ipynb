{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training HMMs when hidden states are truly unobserved\n",
    "\n",
    "In this case we use a expectation maximization (EM) algorithm. The EM algorithm is useful whenever we have to marginalize over an unknown (hidden) variable. Our likelihood is $p(x) = \\sum_{z} p(x, z)$.\n",
    "\n",
    "The E step of the EM algorithm implies computing the analogue of the counting in the case of observed $z$ s. The thing is that, now, these countings depend on $A, B, \\pi, \\alpha$ and $\\beta$ themselves (so we actually need the forward-backward algorithms at this point). The fact that we cannot count directly is caused by the unobserved nature of the $z$ s.\n",
    "\n",
    "E step:\n",
    "$$\n",
    "\\xi_t(i,j) = \\frac{\\alpha_t(i)A_{i,j}B_{j,x_{t+1}}\\beta_{t+1}(j)}{\\sum_{i=1}^M\\sum_{j=1}^M\\alpha_t(i)A_{i,j}B_{j,x_{t+1}}\\beta_{t+1}(j)}\\,,\\\\\n",
    "\n",
    "\\gamma_t(i) = \\sum_{j=1}^M \\xi_t(i,j)\n",
    "$$\n",
    "\n",
    "The M step now computes $\\pi, A$ and $B$ from the quantities $\\xi$ and $\\gamma$\n",
    "\n",
    "M step:\n",
    "\n",
    "$$\n",
    "\\pi_i = \\gamma_1(i)\\,,\\\\\n",
    "A_{i,j} = \\frac{\\sum_{t=1}^{T-1}\\xi_t(i,j)}{\\sum_{t=1}^{T-1}\\gamma_t(i)}\\,,\\\\\n",
    "B_{j,k} = \\frac{\\sum_{t=1}^{T}\\gamma_t(j)\\mathbb{1}(x_t=k)}{\\sum_{t=1}^{T}\\gamma_t(j)}\\,,\n",
    "$$\n",
    "\n",
    "This is an iterative algorithm that starts from guessed values for $A$, $B$ and $\\pi$ and iterates until convergence. The EM algorithm applied to HMMs (the above algorithm) is actually called `Baum-Welch` algorithm. It ensures only convergence to a *local* maximum, and so one could perform more than one training (starting from different initial points in parameter space) and keep the best final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the number of hidden states\n",
    "\n",
    "When states $z$ are really unobserved, we do not know how many hidden states to choose ($M$). This needs to be treated as a hyperparameter of the model. For example, we can choose the $M$ that maximizes the log-lokelihood of the resulting model.\n",
    "\n",
    "Another possibility is to use AIC or BIC, that choose the best model penalizing for the number of parameters (on the training set only!)\n",
    "\n",
    "$$ AIC = 2p- 2 \\log L\\,,\\\\\n",
    "BIC = p\\log N -2\\log L\\,,\n",
    "$$\n",
    "where $p$ is the number of parameters. We can choose the minimum (best) AIC or BIC after evaluating on different $M$s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baum-Welch algorithm for multiple observations\n",
    "\n",
    "We index each observation (each sequence) in our training set by $n$, with $n=1,..., N$. Now we will have $N$ training sequences and, for each one, we will be able to compute a $\\alpha_n\\,, \\beta_n$. There will be also a overall probability of the sequence $P_n$ and a length that of course depends on the sequence (called $T_n$).\n",
    "\n",
    "With this we can use the generalization of our previous formulas (now explicit in $\\alpha, \\beta$) as:\n",
    "$$\n",
    "\\pi_i = \\frac{1}{N}\\sum_{n=1}^N \\frac{\\alpha_n(1,i)\\beta_n(1,i)}{P_n}\\,,\\\\\n",
    "A_{ij} = \\frac{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n-1}\\alpha_n(t,i)A_{ij}B_{j, x_n(t+1)}\\beta_n(t+1,j)}{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n-1}\\alpha_n(t,i)\\beta_n(t,i)}\\,,\\\\\n",
    "B_{jk} = \\frac{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n}\\alpha_n(t,j)\\beta_n(t,j) \\mathbb{1}(x_n(t)=k)}{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n}\\alpha_n(t,j)\\beta_n(t,j)}\\,,\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class HMM that fits the parameters and makes an inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a valid random markov matrix\n",
    "def random_normalized(dim1, dim2):\n",
    "    \"\"\"\n",
    "    Creates a matrix of random numbers that are compatible with Markov\n",
    "    (namely, they rows sum up to 1)\n",
    "    \"\"\"\n",
    "    random_matrix = np.random.rand(dim1, dim2)\n",
    "    #divide by sum over rows to make rows add up to 1 (Markov matrix)\n",
    "    return random_matrix/random_matrix.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HMM\n",
    "class HMM:\n",
    "    def __init__(self, num_hidden_states):\n",
    "        self.M = num_hidden_states\n",
    "\n",
    "    def fit(self, X: np.ndarray, max_iter: int = 30):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            max_iter: maximum number of EM iterations to allow\n",
    "            X: stores a set of observed sequences as rows.\n",
    "        \"\"\"\n",
    "        # include seed for reproducibility\n",
    "        np.random.seed(123)\n",
    "\n",
    "        # vocabulary size (classes from 0 to V-1)\n",
    "        # (number of x states = K is the maximum number \n",
    "        # of states in the training set found for a sequence)\n",
    "        vocab_size = max(max(x) for x in X) + 1 \n",
    "        num_sequences = len(X)\n",
    "\n",
    "        # initialize pi, A and B\n",
    "        self.pi = np.ones(self.M)/self.M # uniform distirbution for pi\n",
    "        self.A = random_normalized(self.M, self.M)\n",
    "        self.B = random_normalized(self.M, vocab_size)\n",
    "        print('initial values:', self.pi, self.A, self.B)\n",
    "        # store cost\n",
    "        costs = list()\n",
    "        for it in range(max_iter):\n",
    "            if it % 10 == 0:\n",
    "                print('it:', it)\n",
    "            alphas = list()\n",
    "            betas = list()\n",
    "            P = np.zeros(num_sequences) # probabilities\n",
    "            # loop through observations\n",
    "            for n in range(num_sequences):\n",
    "                x = X[n] # n-th sequence\n",
    "                T = len(x) # T_n\n",
    "\n",
    "                #### Compute alpha[t]: forward algorithm\n",
    "                alpha = np.zeros((T, self.M))\n",
    "                alpha[0] = self.pi * self.B[:, x[0]]\n",
    "                for t in range(1, T):\n",
    "                    # * is the element by element multiplication\n",
    "                    alpha[t] = alpha[t-1].dot(self.A) * self.B[:, x[t]]\n",
    "                P[n] = alpha[-1].sum() # probability of a sequence\n",
    "                alphas.append(alpha)\n",
    "\n",
    "                #### Compute beta[t]: backward algorithm\n",
    "                beta = np.zeros((T, self.M))\n",
    "                beta[-1] = 1\n",
    "                for t in range(T-2,-1,-1): # go backwards\n",
    "                    beta[t] = self.A.dot(self.B[:, x[t+1]] * beta[t+1])\n",
    "                betas.append(beta)\n",
    "\n",
    "            #compute cost\n",
    "            cost = np.sum(np.log(P))\n",
    "            costs.append(cost)\n",
    "\n",
    "            #### Reestimate pi, A and B\n",
    "            self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(num_sequences))/num_sequences\n",
    "\n",
    "            denominator_1 = np.zeros((self.M,1))\n",
    "            denominator_2 = np.zeros((self.M,1))\n",
    "            a_num = 0\n",
    "            b_num = 0\n",
    "            for n in range(num_sequences):\n",
    "                x = X[n] # sequence\n",
    "                T = len(x) # T_n \n",
    "\n",
    "                denominator_1 += (alphas[n][:-1] * betas[n][:-1]).sum(axis=0, keepdims=True).T/P[n]\n",
    "                denominator_2 += (alphas[n] * betas[n]).sum(axis=0, keepdims=True).T/P[n]\n",
    "\n",
    "                # nth update for A numerator\n",
    "                a_num_n = np.zeros((self.M, self.M))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(self.M):\n",
    "                        for t in range(T-1):\n",
    "                            a_num_n[i,j] += alphas[n][t,i]*self.A[i,j]*self.B[j,x[t+1]] * betas[n][t+1,j]\n",
    "                a_num += a_num_n/P[n]\n",
    "\n",
    "                # nth update for B numerator\n",
    "                b_num_n = np.zeros((self.M, vocab_size))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(vocab_size):\n",
    "                        for t in range(T):\n",
    "                            if x[t] == j:\n",
    "                                b_num_n[i,j] += alphas[n][t,i] * betas[n][t,i]\n",
    "                b_num += b_num_n/P[n]\n",
    "\n",
    "            # update A and B\n",
    "            self.A = a_num/denominator_1\n",
    "            self.B = b_num/denominator_2\n",
    "\n",
    "        ## print & plot final estimates/costs\n",
    "        print('ITERATION:', it)\n",
    "        print(\"A:\", self.A)\n",
    "        print('check A:', self.A.sum(axis=1))\n",
    "        print(\"B:\", self.B)\n",
    "        print('check B:', self.B.sum(axis=1))\n",
    "        print(\"pi:\", self.pi)\n",
    "        print('check pi:', self.pi.sum())\n",
    "\n",
    "\n",
    "        # plot costs\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def likelihood(self, x):\n",
    "        '''\n",
    "        Computes the probability (likelihood) of a sequence\n",
    "        by means of the forwards algorithm\n",
    "        '''\n",
    "        T = len(x) # T_n\n",
    "\n",
    "        #### Compute alpha[t]: forward algorithm\n",
    "        alpha = np.zeros((T, self.M))\n",
    "        alpha[0] = self.pi * self.B[:, x[0]]\n",
    "        for t in range(1, T):\n",
    "            # * is the element by element multiplication\n",
    "            alpha[t] = alpha[t-1].dot(self.A) * self.B[:, x[t]]\n",
    "        return alpha[-1].sum()\n",
    "\n",
    "    def likelihood_multi(self, X):\n",
    "        '''\n",
    "        Computes the probability (likelihood) for all the\n",
    "        observations (sequences)\n",
    "        '''\n",
    "        return np.array([self.likelihood(x) for x in X])\n",
    "\n",
    "    def log_likelihood_multi(self, X):\n",
    "        '''\n",
    "        Computes the log likelihood of all the observations\n",
    "        '''\n",
    "        return np.log(self.likelihood_multi(X))\n",
    "\n",
    "    def get_state_sequence(self, x):\n",
    "        '''\n",
    "        Computes the most probable set of hidden states given \n",
    "        observation sequence x using the Viterbi algorithm\n",
    "        '''\n",
    "        T = len(x)\n",
    "        delta = np.zeros((T, self.M))\n",
    "        psi = np.zeros((T, self.M))\n",
    "        delta[0] = self.pi * self.B[:, x[0]]\n",
    "        for t in range(1,T):\n",
    "            for j in range(self.M):\n",
    "                delta[t,j] = np.max(delta[t-1]*self.A[:,j])*self.B[j, x[t]]\n",
    "                psi[t,j] = np.argmax(delta[t-1]*self.A[:,j])\n",
    "\n",
    "        ### Backtrack\n",
    "        states = np.zeros(T, dtype=np.int32)\n",
    "        states[T-1] = np.argmax(delta[T-1])\n",
    "        for t in range(T-2,-1,-1):\n",
    "            states[t] = psi[t+1, states[t+1]]\n",
    "\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the HMM in a dataset of sequences of coin flips with heads (`H`) or tails (`T`) generated with the following parameters:\n",
    "\n",
    "$$\n",
    "pi = (0.5,0.5).T\\\\\n",
    "A = \\begin{pmatrix}\n",
    "0.1 & 0.9 \\\\\n",
    "0.8 & 0.2 \n",
    "\\end{pmatrix}\\\\\n",
    "\n",
    "B = \\begin{pmatrix}\n",
    "0.6 & 0.4\\\\\n",
    "0.3 & 0.7\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The file is called `coin_data.txt` and can be found in \n",
    "\n",
    "https://github.com/lazyprogrammer/machine_learning_examples/blob/master/hmm_class/coin_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_coin():\n",
    "    '''\n",
    "    Create training set from coin_data.txt file\n",
    "    and fit a HMM with it\n",
    "    '''\n",
    "    X = list()\n",
    "    for line in open('coin_data.txt'):\n",
    "        x = [1 if elm == 'H' else 0 for elm in line.rstrip()]\n",
    "        X.append(x)\n",
    "\n",
    "    hmm = HMM(2)\n",
    "    hmm.fit(X,max_iter=100)\n",
    "    L = hmm.log_likelihood_multi(X).sum()\n",
    "\n",
    "    print(\"log likelihood after fitting:\", L)\n",
    "    \n",
    "    # set HMM to the actual values that generated the series\n",
    "    # of coin flips\n",
    "    hmm.pi = np.array([0.5, 0.5])\n",
    "    hmm.A = np.array([[0.1, 0.9], [0.8, 0.2]])\n",
    "    hmm.B = np.array([[0.6, 0.4], [0.3, 0.7]])\n",
    "    L = hmm.log_likelihood_multi(X).sum()\n",
    "    print(\"log likelihood of real generating model:\", L)\n",
    "\n",
    "    # try Viterbi on training set\n",
    "    print(\"best state sequence for \", X[0])\n",
    "    print(hmm.get_state_sequence(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial values: [0.5 0.5] [[0.7087962  0.2912038 ]\n",
      " [0.29152056 0.70847944]] [[0.62969057 0.37030943]\n",
      " [0.58883752 0.41116248]]\n",
      "it: 0\n",
      "it: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1644/389656226.py:60: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(num_sequences))/num_sequences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 20\n",
      "it: 30\n",
      "it: 40\n",
      "it: 50\n",
      "it: 60\n",
      "it: 70\n",
      "it: 80\n",
      "it: 90\n",
      "ITERATION: 99\n",
      "A: [[0.70371831 0.29628169]\n",
      " [0.28697428 0.71302572]]\n",
      "check A: [1. 1.]\n",
      "B: [[0.54110179 0.45889821]\n",
      " [0.54024348 0.45975652]]\n",
      "check B: [1. 1.]\n",
      "pi: [0.51003807 0.48996193]\n",
      "check pi: 1.0000000000000002\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood after fitting: -1034.7539242965554\n",
      "log likelihood of real generating model: -1059.7229160265022\n",
      "best state sequence for  [0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "fit_coin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the above algorithms is *underflow*: after all, $\\alpha$ and $\\beta$ are small numbers ($<1$) and hence, for long sequences, the training algorithm will lead to zero for $\\alpha$ really soon, meaning that all multiplications will be zero from that point on. To solve this issue, we will do the following:\n",
    "\n",
    "- For the Viterbi algorithm we will use logs, since it only involve multiplications. Hence, we turn the multiplications into sums and avoid the underflow problem.\n",
    "\n",
    "- For the Baum-Welch algorithm, we will use scaling. For this, we will use a scale factor $c_t\\,, t=1,...,T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19f2dcfaf593399b30a8f53af9c897705b95d8f5016d0f862fa64694b4ff6a68"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('HMMs_udemy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

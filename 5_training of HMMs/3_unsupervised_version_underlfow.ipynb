{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The underflow problem of HMMs\n",
    "\n",
    "There is an computational issue with the use of the EM algorithm for training HMMs, and this is called the underflow problem.\n",
    "The issue has to do with the fact that, in the recursive algorithms, we are repeatedly multiplying small numbers by themselves many times. Think of the forward algorithm: if $\\alpha <1$, the fact that $\\alpha(t)$ is computed by multiplying $\\alpha(t-1)$ recursively implies that a long sequence (of order hundred elements) will eventually produce an outcome of $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi algorithm\n",
    "\n",
    "Viterbi is easy because it only involves multiplications. To solve the underflow problem in this case it is therefore enough to consider the $\\log$ of the different quantities instead of the quntities themselves, as $$ \\log(AB) = \\log(A) + \\log(B)\\,,$$ which solves the underlfow problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling for Forward algorithm\n",
    "\n",
    "Contrary to Viterbi, the forward, backward and also training algorithms involve multiplications but also sums, and therefore taking the $\\log$ is not of any use.\n",
    "\n",
    "The useful concept here involves **scaling**. To that end, we include a *scale factor* $$c(t),\\ t=1,...\\,.T$$ Now, the forward algorithm can be rewritten as\n",
    "\n",
    "$$ \\alpha'(1, i) = \\pi_i B(j, x(1)) = \\alpha(t, i) $$\n",
    "$$ c(t) = \\sum_{i=1}^M \\alpha'(t,i)$$\n",
    "$$\\hat{\\alpha}(t,i) = \\frac{\\alpha'(t, i)}{c(t)}$$\n",
    "\n",
    "In particular $\\hat{\\alpha}(1,i) = \\pi_i B(j, x(1))/c(1)$. Also, the induction step involves $\\alpha'$ and $\\hat{\\alpha}$ now\n",
    "\n",
    "$$ \\alpha'(t, i) = \\sum_{i=1}^M \\hat{\\alpha}(t-1, i) A(i,j)B(j,x(t))\\,,$$\n",
    "\n",
    "The probability of a sequence now becomes $$p(x_1,...,x_T) = \\prod_{t=1}^T c(t)\\,.$$ Now we have accomplished the desire expression of the probability **solely as a product**, not as a sum or combinations of sumas and products. Therefore, to avoid the underlfow problem, it is now as easy as taking the $\\log$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling for Backward algorithm\n",
    "\n",
    "For the backward algorithm ($\\beta$ update) it is not necessary to compute a new scale factor. We use the previous one $c(t)$.\n",
    "\n",
    "The initialization step is still $$\\hat{\\beta}(T,i)=1\\,.$$\n",
    "\n",
    "The induction step involves the scaling $$ \\hat{\\beta}(t,i) = \\frac{\\sum_{j=1}^N A(i,j)B(j, x(t+1)\\hat{\\beta}(t+1, j))}{c(t+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important identities\n",
    "\n",
    "These will be used when recasting the update equations in terms of hatted variables.\n",
    "\n",
    "$$ \\left(\\prod_{\\tau=1}^T c(\\tau)\\right)\\hat{\\alpha}(t,i) = \\alpha(t,i)\\,,$$\n",
    "$$ \\hat{\\alpha}(t,i)\\hat{\\beta}(t,i) p(x) = \\alpha(t,i)\\beta(t,i) \\,,$$\n",
    "$$ \\alpha(t,i)\\beta(t+1,j) = \\frac{\\hat{\\alpha}(t,i)\\hat{\\beta}(t+1,j) p(x)}{c(t+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update equations of the EM algorithm with scaling\n",
    "\n",
    "$$ \\pi_i = \\frac{1}{N} \\sum_{n=1}^N \\hat{\\alpha}_n(1,i)\\hat{\\beta}_n(1,i)$$\n",
    "$$ A(i,j) = \\frac{\\sum_{n=1}^N \\sum_{t=1}^{T_n-1} \\frac{\\hat{\\alpha}_n(t,i)\\hat{\\beta}_n(t+1,j)}{c(t+1)} A(i,j) B(j, x_n(t+1))}{\\sum_{n=1}^N \\sum_{t=1}^{T_n-1} \\hat{\\alpha}_n(t,i)\\hat{\\beta}_n(t,i)}$$\n",
    "$$ B(j,k) = \\frac{\\sum_{n=1}^N \\sum_{t=1}^{T_n} \\hat{\\alpha}_n(t,j)\\hat{\\beta}_n(t,j) \\text{  if  } x_n(t)=k \\text{  else  } 0}{\\sum_{n=1}^N \\sum_{t=1}^{T_n} \\hat{\\alpha}_n(t,j)\\hat{\\beta}_n(t,j)}$$\n",
    "\n",
    "The probability of observation is now implicitly included in the hatted variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class HMM that fits the parameters and makes an inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a valid random markov matrix\n",
    "def random_normalized(dim1, dim2):\n",
    "    \"\"\"\n",
    "    Creates a matrix of random numbers that are compatible with Markov\n",
    "    (namely, they rows sum up to 1)\n",
    "    \"\"\"\n",
    "    random_matrix = np.random.rand(dim1, dim2)\n",
    "    #divide by sum over rows to make rows add up to 1 (Markov matrix)\n",
    "    return random_matrix/random_matrix.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HMM\n",
    "class HMM:\n",
    "    def __init__(self, num_hidden_states):\n",
    "        self.M = num_hidden_states\n",
    "\n",
    "    def fit(self, X: np.ndarray, max_iter: int = 30, verbose: bool=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            max_iter: maximum number of EM iterations to allow\n",
    "            X: stores a set of observed sequences as rows.\n",
    "        \"\"\"\n",
    "        # include seed for reproducibility\n",
    "        np.random.seed(123)\n",
    "\n",
    "        # vocabulary size (classes from 0 to V-1)\n",
    "        # (number of x states = K is the maximum number \n",
    "        # of states in the training set found for a sequence)\n",
    "        vocab_size = max(max(x) for x in X) + 1 \n",
    "        num_sequences = len(X)\n",
    "\n",
    "        # initialize pi, A and B\n",
    "        self.pi = np.ones(self.M)/self.M # uniform distirbution for pi\n",
    "        self.A = random_normalized(self.M, self.M)\n",
    "        self.B = random_normalized(self.M, vocab_size)\n",
    "        # store cost\n",
    "        costs = list()\n",
    "        for it in range(max_iter):\n",
    "            #if it % 10 == 0:\n",
    "            #    print('it:', it)\n",
    "            alphas = list()\n",
    "            betas = list()\n",
    "            P = np.zeros(num_sequences) # probabilities\n",
    "            # loop through observations\n",
    "            for n in range(num_sequences):\n",
    "                x = X[n] # n-th sequence\n",
    "                T = len(x) # T_n\n",
    "\n",
    "                #### Compute alpha[t]: forward algorithm\n",
    "                alpha = np.zeros((T, self.M))\n",
    "                alpha[0] = self.pi * self.B[:, x[0]]\n",
    "                for t in range(1, T):\n",
    "                    # * is the element by element multiplication\n",
    "                    alpha[t] = alpha[t-1].dot(self.A) * self.B[:, x[t]]\n",
    "                P[n] = alpha[-1].sum() # probability of a sequence\n",
    "                alphas.append(alpha)\n",
    "\n",
    "                #### Compute beta[t]: backward algorithm\n",
    "                beta = np.zeros((T, self.M))\n",
    "                beta[-1] = 1\n",
    "                for t in range(T-2,-1,-1): # go backwards\n",
    "                    beta[t] = self.A.dot(self.B[:, x[t+1]] * beta[t+1])\n",
    "                betas.append(beta)\n",
    "\n",
    "            #compute cost\n",
    "            cost = np.sum(np.log(P))\n",
    "            costs.append(cost)\n",
    "\n",
    "            #### Reestimate pi, A and B\n",
    "            self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(num_sequences))/num_sequences\n",
    "\n",
    "            denominator_1 = np.zeros((self.M,1))\n",
    "            denominator_2 = np.zeros((self.M,1))\n",
    "            a_num = 0\n",
    "            b_num = 0\n",
    "            for n in range(num_sequences):\n",
    "                x = X[n] # sequence\n",
    "                T = len(x) # T_n \n",
    "\n",
    "                denominator_1 += (alphas[n][:-1] * betas[n][:-1]).sum(axis=0, keepdims=True).T/P[n]\n",
    "                denominator_2 += (alphas[n] * betas[n]).sum(axis=0, keepdims=True).T/P[n]\n",
    "\n",
    "                # nth update for A numerator\n",
    "                a_num_n = np.zeros((self.M, self.M))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(self.M):\n",
    "                        for t in range(T-1):\n",
    "                            a_num_n[i,j] += alphas[n][t,i]*self.A[i,j]*self.B[j,x[t+1]] * betas[n][t+1,j]\n",
    "                a_num += a_num_n/P[n]\n",
    "\n",
    "                # nth update for B numerator\n",
    "                b_num_n = np.zeros((self.M, vocab_size))\n",
    "                for i in range(self.M):\n",
    "                    for j in range(vocab_size):\n",
    "                        for t in range(T):\n",
    "                            if x[t] == j:\n",
    "                                b_num_n[i,j] += alphas[n][t,i] * betas[n][t,i]\n",
    "                b_num += b_num_n/P[n]\n",
    "\n",
    "            # update A and B\n",
    "            self.A = a_num/denominator_1\n",
    "            self.B = b_num/denominator_2\n",
    "\n",
    "            #self.alphas = alphas\n",
    "            #self.betas = betas\n",
    "            ## print & plot final estimates/costs\n",
    "            if verbose:\n",
    "                print('ITERATION:', it)\n",
    "                print(\"A:\", self.A)\n",
    "                print('check A:', self.A.sum(axis=1))\n",
    "                print(\"B:\", self.B)\n",
    "                print('check B:', self.B.sum(axis=1))\n",
    "                print(\"pi:\", self.pi)\n",
    "                print('check pi:', self.pi.sum())\n",
    "        self.alphas = alphas\n",
    "        self.betas = betas\n",
    "\n",
    "        # plot costs\n",
    "        #plt.plot(costs)\n",
    "        #plt.show()\n",
    "\n",
    "    def likelihood(self, x):\n",
    "        '''\n",
    "        Computes the probability (likelihood) of a sequence\n",
    "        by means of the forwards algorithm\n",
    "        '''\n",
    "        T = len(x) # T_n\n",
    "\n",
    "        #### Compute alpha[t]: forward algorithm\n",
    "        alpha = np.zeros((T, self.M))\n",
    "        alpha[0] = self.pi * self.B[:, x[0]]\n",
    "        for t in range(1, T):\n",
    "            # * is the element by element multiplication\n",
    "            alpha[t] = alpha[t-1].dot(self.A) * self.B[:, x[t]]\n",
    "        return alpha[-1].sum()\n",
    "\n",
    "    def likelihood_multi(self, X):\n",
    "        '''\n",
    "        Computes the probability (likelihood) for all the\n",
    "        observations (sequences)\n",
    "        '''\n",
    "        return np.array([self.likelihood(x) for x in X])\n",
    "\n",
    "    def log_likelihood_multi(self, X):\n",
    "        '''\n",
    "        Computes the log likelihood of all the observations\n",
    "        '''\n",
    "        return np.log(self.likelihood_multi(X))\n",
    "\n",
    "    def get_state_sequence(self, x):\n",
    "        '''\n",
    "        Computes the most probable set of hidden states given \n",
    "        observation sequence x using the Viterbi algorithm\n",
    "        '''\n",
    "        T = len(x)\n",
    "        delta = np.zeros((T, self.M))\n",
    "        psi = np.zeros((T, self.M))\n",
    "        delta[0] = self.pi * self.B[:, x[0]]\n",
    "        for t in range(1,T):\n",
    "            for j in range(self.M):\n",
    "                delta[t,j] = np.max(delta[t-1]*self.A[:,j])*self.B[j, x[t]]\n",
    "                psi[t,j] = np.argmax(delta[t-1]*self.A[:,j])\n",
    "\n",
    "        ### Backtrack\n",
    "        states = np.zeros(T, dtype=np.int32)\n",
    "        states[T-1] = np.argmax(delta[T-1])\n",
    "        for t in range(T-2,-1,-1):\n",
    "            states[t] = psi[t+1, states[t+1]]\n",
    "\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the HMM in a dataset of sequences of coin flips with heads (`H`) or tails (`T`) generated with the following parameters:\n",
    "\n",
    "$$\n",
    "\\pi = (0.5,0.5).T\\\\\n",
    "A = \\begin{pmatrix}\n",
    "0.1 & 0.9 \\\\\n",
    "0.8 & 0.2 \n",
    "\\end{pmatrix}\\\\\n",
    "\n",
    "B = \\begin{pmatrix}\n",
    "0.6 & 0.4\\\\\n",
    "0.3 & 0.7\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The file is called `coin_data.txt` and can be found in \n",
    "\n",
    "https://github.com/lazyprogrammer/machine_learning_examples/blob/master/hmm_class/coin_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_coin():\n",
    "    '''\n",
    "    Create training set from coin_data.txt file\n",
    "    and fit a HMM with it\n",
    "    '''\n",
    "    X = list()\n",
    "    for line in open('coin_data.txt'):\n",
    "        x = [1 if elm == 'H' else 0 for elm in line.rstrip()]\n",
    "        X.append(x)\n",
    "\n",
    "    hmm = HMM(2)\n",
    "    hmm.fit(X,max_iter=100)\n",
    "    L = hmm.log_likelihood_multi(X).sum()\n",
    "\n",
    "    print(\"log likelihood after fitting:\", L)\n",
    "    \n",
    "    # set HMM to the actual values that generated the series\n",
    "    # of coin flips\n",
    "    hmm.pi = np.array([0.5, 0.5])\n",
    "    hmm.A = np.array([[0.1, 0.9], [0.8, 0.2]])\n",
    "    hmm.B = np.array([[0.6, 0.4], [0.3, 0.7]])\n",
    "    L = hmm.log_likelihood_multi(X).sum()\n",
    "    print(\"log likelihood of real generating model:\", L)\n",
    "\n",
    "    # try Viterbi on training set\n",
    "    print(\"best state sequence for \", X[0])\n",
    "    print(hmm.get_state_sequence(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial values: [0.5 0.5] [[0.7087962  0.2912038 ]\n",
      " [0.29152056 0.70847944]] [[0.62969057 0.37030943]\n",
      " [0.58883752 0.41116248]]\n",
      "it: 0\n",
      "it: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1644/389656226.py:60: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(num_sequences))/num_sequences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 20\n",
      "it: 30\n",
      "it: 40\n",
      "it: 50\n",
      "it: 60\n",
      "it: 70\n",
      "it: 80\n",
      "it: 90\n",
      "ITERATION: 99\n",
      "A: [[0.70371831 0.29628169]\n",
      " [0.28697428 0.71302572]]\n",
      "check A: [1. 1.]\n",
      "B: [[0.54110179 0.45889821]\n",
      " [0.54024348 0.45975652]]\n",
      "check B: [1. 1.]\n",
      "pi: [0.51003807 0.48996193]\n",
      "check pi: 1.0000000000000002\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelihood after fitting: -1034.7539242965554\n",
      "log likelihood of real generating model: -1059.7229160265022\n",
      "best state sequence for  [0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "fit_coin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the above algorithms is *underflow*: after all, $\\alpha$ and $\\beta$ are small numbers ($<1$) and hence, for long sequences, the training algorithm will lead to zero for $\\alpha$ really soon, meaning that all multiplications will be zero from that point on. To solve this issue, we will do the following:\n",
    "\n",
    "- For the Viterbi algorithm we will use logs, since it only involve multiplications. Hence, we turn the multiplications into sums and avoid the underflow problem.\n",
    "\n",
    "- For the Baum-Welch algorithm, we will use scaling. For this, we will use a scale factor $c_t\\,, t=1,...,T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorization\n",
    "\n",
    "Trying to run the forward and backward algorithms in general for an arbitrary sequence of length 30 (such as the one here) leads to an out-of-memory situation. \n",
    "We will therefore substitute the value of the sequence at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a valid random markov matrix\n",
    "def random_normalized(dim1, dim2):\n",
    "    \"\"\"\n",
    "    Creates a matrix of random numbers that are compatible with Markov\n",
    "    (namely, they rows sum up to 1)\n",
    "    \"\"\"\n",
    "    random_matrix = np.random.rand(dim1, dim2)\n",
    "    #divide by sum over rows to make rows add up to 1 (Markov matrix)\n",
    "    return random_matrix/random_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "def load_data(data_path='./coin_data.txt'):\n",
    "    X = list()\n",
    "    for line in open(data_path):\n",
    "        x = [1 if elm == 'H' else 0 for elm in line.rstrip()]\n",
    "        X.append(x)\n",
    "    return np.array(X)\n",
    "\n",
    "def generate_initial_parameters(vocab_size,\n",
    "                                num_hidden_states,\n",
    "                                random_seed=True):\n",
    "    if random_seed:\n",
    "        np.random.seed(123)\n",
    "\n",
    "    # initialize pi, A and B\n",
    "    pi = (np.ones(num_hidden_states)/num_hidden_states).reshape(num_hidden_states,1) # uniform distirbution for pi\n",
    "    A = random_normalized(num_hidden_states,num_hidden_states)\n",
    "    B = random_normalized(num_hidden_states, vocab_size)\n",
    "\n",
    "    return pi, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 30)\n"
     ]
    }
   ],
   "source": [
    "X_data = load_data()\n",
    "print(X_data.shape) # 50 samples of 30 timesteps each\n",
    "VOCAB_SIZE = max(max(x) for x in X_data) + 1 # 2 different observable states\n",
    "NUM_SEQS, SEQ_LENGTH = X_data.shape\n",
    "M_ = 2 # assume 2-dim hidden space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward algotihm for fixed input sequence\n",
    "\n",
    "Note for example that $$ \\alpha(t=1, j) = \\pi_j B(j, x_1)\\,,$$ where $x_1$ is just the observation associated to the first timestep. We will substitute explicitly this observation. Now, our dataset does not contain only one observation, but $N = 50$ of them. To vectorize everything, we will substitute the $N$ samples of $x_1$ observed.\n",
    "\n",
    "This logic will be applied to the recursive algorithm as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_initial(input_sequences: np.ndarray, \n",
    "                    pi: Union[List, np.ndarray], \n",
    "                    B: Union[List, np.ndarray]) -> np.ndarray:\n",
    "    '''\n",
    "    Computes the value of alpha associated to initial time t=1\n",
    "    \n",
    "    Parameters\n",
    "        input_sequences: np.ndarray\n",
    "            Input set of sequences, i.e. the training data\n",
    "        pi: Union[List, np.ndarray]\n",
    "            Initial vector of probabilities (i.e. p(z_1)) for each possible state of z_1\n",
    "        B:  Union[List, np.ndarray]\n",
    "            time-independent matrix of transition probabiltiies p(x_t|z_t) for each x- and z-states\n",
    "\n",
    "    Return\n",
    "        np.ndarray: initial value of alpha (namely, alpha_1)\n",
    "    '''\n",
    "    assert np.isclose(np.sum(pi), 1), \"the sum of probabilities of initial latent states must add up to 1\"\n",
    "    assert (np.isclose(B.sum(axis=1), 1)).all(), \"the total probability of a given observed state must be 1\"\n",
    "    # element-wise product with B evaluated on x(t=1) for all the rows (all the samples) at once\n",
    "    # alpha_initial will end up having dimension M x N, where N is the total number of samples\n",
    "    return np.multiply(pi, B[:,X_data[:,0]])\n",
    "\n",
    "def beta_initial(A_dim: int, num_sequences: int) -> np.ndarray:\n",
    "    '''\n",
    "    Computes the value of beta associated to initial time t=T\n",
    "\n",
    "    Parameters\n",
    "        A_dim: int\n",
    "            dimension of matrix A (namely, M in the above notation)\n",
    "\n",
    "    Return\n",
    "        np.ndarray: initial value of beta (namely, beta_1)\n",
    "    '''\n",
    "    return np.ones(shape=(A_dim, num_sequences)) # column vector of ones (one component per each z-state)\n",
    "\n",
    "\n",
    "def forward_algo_input_fixed(\n",
    "                    input_sequences: np.ndarray, \n",
    "                    A: Union[List, np.ndarray], \n",
    "                    B: Union[List, np.ndarray], \n",
    "                    alpha_ini: np.ndarray\n",
    "                    ) -> List[np.ndarray]:\n",
    "\n",
    "    ''' \n",
    "    This is the forward algorithm as implemented in notebook forward_algo.ipynb\n",
    "    but with the input sequences evaluated whenever possible to make the problem\n",
    "    computationally light\n",
    "\n",
    "    Parameters\n",
    "        input_sequences: np.ndarray\n",
    "            Input set of sequences, i.e. the training data\n",
    "        A:  Union[List, np.ndarray]\n",
    "            Initial vector of probabilities (i.e. p(z_1)) for each possible state of z_1\n",
    "        B:  Union[List, np.ndarray]\n",
    "            time-independent matrix of transition probabiltiies p(x_t|z_t) for each x- and z-states\n",
    "        alpha_ini: np,ndarray\n",
    "            The value of alpha(t=1, z_1)\n",
    "    Return\n",
    "        List[np.ndarray]: list of alphas of the form \n",
    "        [alpha(t=1, z_1),..., alpha(t=T-1, z_{T-1}), alpha(t=T, z_T)]\n",
    "    '''\n",
    "    sequence_length = input_sequences.shape[1]\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    assert isinstance(sequence_length, int), 'the length of the sequence must be an integer number'\n",
    "    assert alpha_ini.shape[0] == A.shape[0], 'the length of the first axis of alpha_ini must be the rows of A'\n",
    "\n",
    "    alphas_list = [alpha_ini] \n",
    "    for t_ in range(1,sequence_length):\n",
    "        # np.einsum('ij,il->jl', A, alphas_list[t_-1]) gives the first part of the algorithm\n",
    "        # factor1_jt = sum_i A_ij alpha(i,t) -> shape M x N\n",
    "        # The second part of the formula np.multiply(B[:, input_sequences[:,t_]], factor1)\n",
    "        # gives B(j, x_t) * factor1_jt\n",
    "        alphas_list.append(np.multiply(B[:, input_sequences[:,t_]], \n",
    "                                        np.einsum('ij,il->jl', A, alphas_list[t_-1])))\n",
    "    #alpha list has shape T x M x N\n",
    "    return np.array(alphas_list)\n",
    "\n",
    "def backward_algo_input_fixed(input_sequences: np.ndarray, \n",
    "                    A: Union[List, np.ndarray], \n",
    "                    B: Union[List, np.ndarray], \n",
    "                    beta_ini: np.ndarray\n",
    "                    ) -> List[np.ndarray]:\n",
    "\n",
    "    '''\n",
    "    Computes a set of alphas following the (recursive) forward algorithm\n",
    "\n",
    "    Parameters\n",
    "        input_sequences: np.ndarray\n",
    "            Input set of sequences, i.e. the training data\n",
    "        A:  Union[List, np.ndarray]\n",
    "            Initial vector of probabilities (i.e. p(z_1)) for each possible state of z_1\n",
    "        B:  Union[List, np.ndarray]\n",
    "            time-independent matrix of transition probabiltiies p(x_t|z_t) for each x- and z-states\n",
    "        alpha_ini: np,ndarray\n",
    "            The value of alpha(t=1, z_1)\n",
    "    Return\n",
    "        List[np.ndarray]: list of betas of the form \n",
    "        [beta(t=1, z_1), ..., beta(t=T-1, z_{T-1}), beta(t=T, z_T)]\n",
    "    '''\n",
    "    sequence_length = input_sequences.shape[1]\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    assert isinstance(sequence_length, int), 'the length of the sequence must be an integer number'\n",
    "    assert beta_ini.shape[0] == A.shape[0], 'the length of the first axis of beta_ini must be the rows of A'\n",
    "\n",
    "    # initialize betas list\n",
    "    betas_list = [0]*sequence_length\n",
    "    betas_list[-1] = beta_ini #beta_ini will be a M x N matrix of 1s\n",
    "    for t_ in reversed(range(1,sequence_length)):\n",
    "        # note that we don't use t+1 when indexing beta because \n",
    "        # python starts counting indexes from 0: reversed(range(0,sequence_length))\n",
    "        # goes from sequence_length-1 to 0. The last index of betas_list\n",
    "        # is also sequence_length-1\n",
    "        exps1 = np.einsum('ij,js,jl->isl', A, B[:, input_sequences[:,t_]], betas_list[t_])\n",
    "        # force (column of B) = (column of betas_list[t_]) = x_{t+1}  \n",
    "        # i.e pick diagonal for first and last indices: s = l above, without summing over s!\n",
    "        betas_list[t_-1] = np.diagonal(exps1, axis1=len(exps1.shape)-2, axis2=len(exps1.shape)-1)\n",
    "        # note that index associated to z is first index of beta\n",
    "        # (this is so also for beta_ini)\n",
    "    return np.array(betas_list)\n",
    "\n",
    "def probability_fixed_input_sequence(alpha: np.ndarray):\n",
    "    '''\n",
    "    Computes the probability of a fixed sequence of x states given an alpha \n",
    "    (this alpha is suppossed to correspond to alpha(t=T, z_T))\n",
    "\n",
    "    Parameters:\n",
    "        alpha: np.ndarray\n",
    "            A tensor with dimensions (M, K, K,...,K)\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            A tensor with dimension (K, K,...,K)\n",
    "    '''\n",
    "    # sum alpha along the first direction (that would correspond to z_T)\n",
    "    # returns probability for arbitrary set of x\n",
    "    # (note the ordering: ijk... will correspond to x_T, x_{T-1}, x_{T-2},...)\n",
    "    #return alpha.sum(axis=0)\n",
    "    return np.einsum('i...->...', alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_, A_, B_ = generate_initial_parameters(vocab_size=VOCAB_SIZE, num_hidden_states=M_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Forward algorithm**\n",
    "\n",
    "For a given sequence (given row) \n",
    "\n",
    "$$B(i,x_2) \\rightarrow M \\times 1$$\n",
    "$$ \\alpha(t=1,i) \\rightarrow M \\times 1$$\n",
    "$$ A(j,i) \\rightarrow M \\times M$$\n",
    "Recall that:\n",
    "$$\\alpha(t+1, i) = B(i, x_{t+1}) \\sum_j^M A(j,i) \\alpha(t, j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ini = alpha_initial(X_data, pi_, B_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17681246697497588"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: \n",
    "# third sequence (n=3) only for z=2 (i.e. alpha(t=2, z=2))\n",
    "assd=0\n",
    "for i in range(2):\n",
    "    assd += A_[i,1]*alpha_ini[i,2] #z, t and n start from 0!\n",
    "\n",
    "B_[1, X_data[2,1]]*assd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17681246697497588"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the full list of alphas for all N\n",
    "alpha_list = forward_algo_input_fixed(X_data, A_, B_, alpha_ini)\n",
    "alpha_list[1][1,2] #alpha for t=2, z=2 and n=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert B_[1, X_data[2,1]]*assd == alpha_list[1][1,2] #alpha for t=2, z=2 and n=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (probability_fixed_input_sequence(alpha_list[-1]) > 0).all()\n",
    "assert (probability_fixed_input_sequence(alpha_list[-1]) < 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = probability_fixed_input_sequence(alpha_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Backward algorithm**\n",
    "\n",
    "For a given sequence \n",
    "\n",
    "$$B(i,x_{29}) \\rightarrow M \\times 1$$\n",
    "$$ \\beta(t=T,i) \\rightarrow M \\times 1$$\n",
    "$$ A(j,i) \\rightarrow M \\times M$$\n",
    "Recall that:\n",
    "$$\\beta(t, i) = \\sum_j^M A(i,j)B(j, x_{t+1})\\beta(t+1, j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_ini = beta_initial(M_, NUM_SEQS)\n",
    "beta_ini.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6007470210010455"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z=2, n=3, t=29 from  beta_ini (recall that beta_ini corresponds to t=T=30)\n",
    "basse = 0\n",
    "for j in range(M_):\n",
    "    basse += A_[1,j]*B_[j, X_data[2,-1]]*beta_ini[j, 2]\n",
    "basse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6007470210010455"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_list = backward_algo_input_fixed(X_data, A_, B_, beta_ini)\n",
    "beta_list[-2][1,2] # dimension T x M x N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert basse == beta_list[-2][1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update $\\pi$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pi(alpha_list: np.ndarray, beta_list: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Formula to update \\pi according to Baum Welch\n",
    "\n",
    "    Three equivalent ways\n",
    "\n",
    "    Method 1: \n",
    "        np.multiply(beta_list[0]/prob,alpha_list[0]).mean(axis=1)\n",
    "        \n",
    "    Method 2:\n",
    "        llo = 0\n",
    "        for n in range(NUM_SEQS):\n",
    "            llo += beta_list[0][:,n] * alpha_list[0][:, n]/prob[n]\n",
    "        llo/NUM_SEQS\n",
    "\n",
    "    Method 3:\n",
    "        ss = np.diag(np.einsum('ij,lj', beta_list[0]/prob, alpha_list[0]))\n",
    "        ss/NUM_SEQS\n",
    "\n",
    "    Parameters:\n",
    "        alpha_list: np.ndarray\n",
    "            A tensor with dimensions (T, M, N)\n",
    "        beta_list: np.ndarray\n",
    "            A tensor with dimensions (T, M, N)\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            A tensor with dimension (M, 1)\n",
    "    '''\n",
    "    prob = probability_fixed_input_sequence(alpha_list[-1])\n",
    "    return (np.multiply(beta_list[0]/prob, alpha_list[0]).mean(axis=1)).reshape(beta_list.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_241/3611249952.py:59: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(num_sequences))/num_sequences\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(2)\n",
    "hmm.pi = pi_\n",
    "hmm.A = A_\n",
    "hmm.B = B_\n",
    "hmm.fit(X_data, max_iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update A\n",
    "\n",
    "$$ \n",
    "\n",
    "A_{ij} = \\frac{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n-1}\\alpha_n(t,i)A_{ij}B_{j, x_n(t+1)}\\beta_n(t+1,j)}{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n-1}\\alpha_n(t,i)\\beta_n(t,i)}\\,,\\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_A(input_sequences: np.ndarray, \n",
    "            alpha_list: np.ndarray, \n",
    "            beta_list: np.ndarray,\n",
    "            A_old: np.ndarray,\n",
    "            B_old: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    prob = probability_fixed_input_sequence(alpha_list[-1])\n",
    "\n",
    "    # be careful with reshape! It does not move axis directly!\n",
    "    # use np.moveaxis to turn shape (a,b,c) into (b,c,a), for example\n",
    "    # turn dimension of B_old[:, input_sequences[:,1:]] into shape of beta_list[1:,:,:]\n",
    "    interm_ = np.moveaxis(B_old[:, input_sequences[:,1:]],-1,0)\n",
    "    # note that in the formula B(j,x_{n,t+1}) involves th eexact same indices as\n",
    "    # beta_n(t,j), so we can multiply element by element\n",
    "    # also divide by P_n straight away (see formula)\n",
    "    beta_B = (interm_*beta_list[1:,:,:]/prob)\n",
    "\n",
    "    # We use the X[1:,:,:] in the indices to denote that what enters is t+1\n",
    "\n",
    "    # A_{ij} does not involve t, n, so we move it away from sums\n",
    "    # sum in t goes from t=1 to T-1. This involves using alpha_list[:-1,:,:]\n",
    "    # (t+1) goes from 1 to T, that explain the indices of B and beta above\n",
    "    # alpha coes with index z=i and beta_B with z=j, that is why those indices\n",
    "    # remain free to match with A_old{ij}\n",
    "    numerator = A_old*np.einsum('ijk,irk->jr', alpha_list[:-1,:,:], beta_B) # dim MxM\n",
    "    '''\n",
    "    Another way:\n",
    "        numerator = np.einsum('ijk,lm,nki,irk->jlmnr', alpha_list[:-1,:,:], A_, B_[:, input_sequences[:,1:]], beta_list[1:,:,:]/prob)\n",
    "        numerator = np.diagonal(numerator, axis1=0, axis2=1)\n",
    "        numerator = np.diagonal(numerator, axis1=0, axis2=1)\n",
    "        numerator = np.diagonal(numerator, axis1=0, axis2=len(numerator.shape)-1)\n",
    "    '''\n",
    "    denominator = np.diag(np.einsum('ikj,isj->ks', alpha_list[:-1,:,:], beta_list[:-1,:,:]/prob)) # dim M \n",
    "\n",
    "    # see formula: we want numerator[i,j]/denominator[i]\n",
    "    #reshape denominator to acquire dimension Mx1\n",
    "    return numerator/denominator.reshape(denominator.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "update_A() missing 2 required positional arguments: 'A_old' and 'B_old'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_241/3789288382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mupdate_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: update_A() missing 2 required positional arguments: 'A_old' and 'B_old'"
     ]
    }
   ],
   "source": [
    "update_A(X_data, alpha_list, beta_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update B\n",
    "\n",
    "$$\n",
    "B_{jk} = \\frac{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n}\\alpha_n(t,j)\\beta_n(t,j) \\mathbb{1}(x_n(t)=k)}{\\sum_{n=1}^N \\frac{1}{P_n} \\sum_{t=1}^{T_n}\\alpha_n(t,j)\\beta_n(t,j)}\\,,\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_B(input_sequences: np.ndarray, \n",
    "            alpha_list: np.ndarray, \n",
    "            beta_list: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    prob = probability_fixed_input_sequence(alpha_list[-1])\n",
    "\n",
    "    vocab_size = max(max(x) for x in input_sequences) + 1\n",
    "    # define delta-type object, that is only 1 whenever k equals x_n(t)\n",
    "    mask_u = list()\n",
    "    for k_ in range(vocab_size):\n",
    "        # place 1. if condition is fullfilled, 0. otherwise\n",
    "        mask_u.append(np.where(input_sequences == k_, 1., 0.))\n",
    "    mask_u = np.array(mask_u) # dim KxNxT\n",
    "\n",
    "    # indices of alpha_list and beta_list coincide, so we can multiply elemen by element\n",
    "    unmasked_num = (alpha_list*beta_list/prob)\n",
    "    # mask numerator with mask_u\n",
    "    numerator = np.einsum('ijk,lki->jl', unmasked_num, mask_u) # dim MxK\n",
    "    #denominator is just the unmasked numerator summed over t and n\n",
    "    denominator = np.einsum('ijk->j', unmasked_num) # dim M \n",
    "\n",
    "    # see formula: we want numerator[j,k]/denominator[j]\n",
    "    #reshape denominator to acquire dimension Mx1\n",
    "    return numerator/denominator.reshape(denominator.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56095853, 0.43904147],\n",
       "       [0.52091592, 0.47908408]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_B(X_data, alpha_list, beta_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorized Viterbi\n",
    "\n",
    "$$ \\delta(t=1, j) = \\pi(j) B(j,x_1)$$\n",
    "\n",
    "$$ \\delta(t, j) = \\text{max}_{i\\in \\{1,...,M\\}} \\left[\\delta(t-1,i) A(i,j)\\right] B(j, x_t)\\,, \\hspace{1cm} j=1,...,M $$\n",
    "\n",
    "$$p^* = \\text{max}_{i\\in \\{1,...,M\\}} \\delta (t=T, j) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(input_sequences: np.ndarray, \n",
    "            pi: np.ndarray,\n",
    "            A: np.ndarray,\n",
    "            B: np.ndarray) -> np.ndarray:\n",
    "    ''''\n",
    "    Apply a tensorized version of the Viterbi algorithm\n",
    "    \n",
    "    '''\n",
    "    sequence_length = input_sequences.shape[1]\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    pi = np.array(pi).reshape(B.shape[0], 1)\n",
    "\n",
    "    # define initial delta\n",
    "    delta_ini = np.multiply(pi, B[:, input_sequences[:, 0]]) # M x N\n",
    "\n",
    "    deltas_list = [delta_ini]\n",
    "    # initial psi = 0 with dim MxN\n",
    "    psis_list = [np.zeros(shape=(B.shape[0], input_sequences.shape[0]))] \n",
    "    for t_ in range(1,sequence_length):\n",
    "        # compute A(i,j)*delta(t-1, i) (no summation!) by first getting all indices\n",
    "        # and then picking the appropriate diagonal (axis=0 (i) with axis=2 (k))\n",
    "        extend_elmnt_prod = np.einsum('ij,kl->ijkl', A_, deltas_list[t_-1])\n",
    "        elmnt_prod = np.diagonal(extend_elmnt_prod, axis1=0, axis2=2) # M(j) x N x M(i)\n",
    "        # get maximum in the direction of i (axis=2)\n",
    "        max_elmnt_prod = np.max(elmnt_prod, axis=2) # M(j) x N\n",
    "        # multiply by B[:, input_sequences[:, t]] (also with dim = M x N)\n",
    "        # and append\n",
    "        deltas_list.append(np.multiply(max_elmnt_prod, \n",
    "                    B[:, input_sequences[:, t_]])) # element appended has dim= M x N\n",
    "        # append psi[t,j]\n",
    "        psis_list.append(np.argmax(elmnt_prod, axis=2)) # M(j) x N\n",
    "    \n",
    "    ### Backtracking algorithm\n",
    "    optim_states = np.zeros_like(input_sequences) # N x T\n",
    "    #[np.zeros(shape=(1,input_sequences.shape[0]))]*sequence_length\n",
    "    #optim_states[-1] = np.argmax(deltas_list[-1], axis=0) # argmax(delta(T)) has dim = N\n",
    "    optim_states[:,-1] = np.argmax(deltas_list[-1], axis=0) # argmax(delta(T)) has dim = N\n",
    "    #return np.array(psis_list), optim_states\n",
    "    # go barckards\n",
    "    for t_ in reversed(range(1,sequence_length)):\n",
    "        # preserve time direction\n",
    "        # first compute general tensor with all indices\n",
    "        # and then pick diagonal: so for example, for t_=29\n",
    "        #you get np.array(psis_list)[29, optim_states[i,29], i] for i=0,...,N-1\n",
    "        optim_states[:, t_-1] = np.diagonal(np.array(psis_list)[t_, \n",
    "                                                    optim_states[:, t_], :]) # diag of N x N matrix\n",
    "\n",
    "    return np.array(deltas_list), np.array(psis_list), optim_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt, psps, opt_st = viterbi(X_data, pi_, A_, B_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = X_data.shape[1]\n",
    "delta = np.zeros((T, 2))\n",
    "psi = np.zeros((T, 2))\n",
    "# for each n\n",
    "for n_ in range(50):\n",
    "    # compare version with for loops...\n",
    "    delta[0] =pi_.reshape(2,) * B_[:, X_data[n_,0]]\n",
    "    for t in range(1,T):\n",
    "        for j in range(2):\n",
    "            delta[t,j] = np.max(delta[t-1]*A_[:,j])*B_[j, X_data[n_,t]]\n",
    "            psi[t,j] = np.argmax(delta[t-1]*A_[:,j])\n",
    "    # ... with tensorized version\n",
    "    ddt_ = np.array(dlt)[:,:,n_]\n",
    "    pss_ = np.array(psps)[:,:,n_]\n",
    "    \n",
    "    if not np.isclose(delta, ddt_).all():\n",
    "        print('DELTA DIFF', n_)\n",
    "    if not np.isclose(psi, pss_).all():\n",
    "        print('PSI DIFF', n_)\n",
    "\n",
    "    ### Backtrack\n",
    "    states = np.zeros(T, dtype=np.int32)\n",
    "    states[T-1] = np.argmax(delta[T-1])\n",
    "    for t in range(T-2,-1,-1):\n",
    "        states[t] = psi[t+1, states[t+1]]\n",
    "\n",
    "    if not np.isclose(states, opt_st[n_,:]).all():\n",
    "        print('STATES DIFF', n_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tensorized HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedHMM:\n",
    "\n",
    "    def __init__(self, num_hidden_states):\n",
    "        self.M = num_hidden_states\n",
    "    @profile\n",
    "    def fit(self, X: np.ndarray, max_iter: int = 30, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            max_iter: maximum number of EM iterations to allow\n",
    "            X: stores a set of observed sequences as rows.\n",
    "        \"\"\"\n",
    "\n",
    "        # include seed for reproducibility\n",
    "        np.random.seed(123)\n",
    "\n",
    "        # vocabulary size (classes from 0 to V-1)\n",
    "        # (number of x states = K is the maximum number \n",
    "        # of states in the training set found for a sequence)\n",
    "        vocab_size = max(max(x) for x in X) + 1 \n",
    "        num_sequences = len(X)\n",
    "\n",
    "        # initialize pi, A and B\n",
    "        self.pi, self.A, self.B = generate_initial_parameters(\n",
    "                                                vocab_size=vocab_size, \n",
    "                                                num_hidden_states=self.M)\n",
    "        # store cost\n",
    "        costs = list()\n",
    "        for it in range(max_iter):\n",
    "            #if it % 10 == 0:\n",
    "            #    print('it:', it)\n",
    "\n",
    "            #### Compute alpha\n",
    "            alpha_ini = alpha_initial(X, self.pi, self.B)\n",
    "            # compute the full list of alphas for all N\n",
    "            self.alpha_list = forward_algo_input_fixed(X, self.A, self.B, alpha_ini)\n",
    "\n",
    "            #### compute beta\n",
    "            beta_ini = beta_initial(self.M, num_sequences)\n",
    "            self.beta_list = backward_algo_input_fixed(X, self.A, self.B, beta_ini)\n",
    "\n",
    "            #### compute cost\n",
    "            P = probability_fixed_input_sequence(self.alpha_list[-1])\n",
    "            cost = np.sum(np.log(P))\n",
    "            costs.append(cost)\n",
    "\n",
    "            #### Reestimate pi, A and B\n",
    "            self.pi = update_pi(self.alpha_list, self.beta_list)\n",
    "            self.A = update_A(X, self.alpha_list, self.beta_list, self.A, self.B)\n",
    "            self.B = update_B(X, self.alpha_list, self.beta_list)\n",
    "\n",
    "            if verbose:\n",
    "                ## print & plot final estimates/costs\n",
    "                print('ITERATION:', it)\n",
    "                print(\"A:\", self.A)\n",
    "                print('check A:', self.A.sum(axis=1))\n",
    "                print(\"B:\", self.B)\n",
    "                print('check B:', self.B.sum(axis=1))\n",
    "                print(\"pi:\", self.pi)\n",
    "                print('check pi:', self.pi.sum())\n",
    "\n",
    "        \n",
    "        # plot costs\n",
    "        #plt.plot(costs)\n",
    "        #plt.show()\n",
    "\n",
    "    def likelihood(self, x):\n",
    "        '''\n",
    "        Computes the probability (likelihood) of a sequence\n",
    "        by means of the forward algorithm\n",
    "        '''\n",
    "        \n",
    "        alpha_ini = alpha_initial(x, self.pi, self.B)\n",
    "        # compute the full list of alphas for all N\n",
    "        self.alpha_list = forward_algo_input_fixed(x, self.A, self.B, alpha_ini)\n",
    "\n",
    "        return probability_fixed_input_sequence(self.alpha_list[-1])\n",
    "\n",
    "    def likelihood_multi(self, X):\n",
    "        '''\n",
    "        Computes the probability (likelihood) for all the\n",
    "        observations (sequences)\n",
    "        '''\n",
    "        #return np.array([self.likelihood(x) for x in X])\n",
    "        return self.likelihood(X)\n",
    "\n",
    "    def log_likelihood_multi(self, X):\n",
    "        '''\n",
    "        Computes the log likelihood of all the observations\n",
    "        '''\n",
    "        return np.log(self.likelihood_multi(X))\n",
    "\n",
    "    def get_state_sequence(self, x):\n",
    "        '''\n",
    "        Computes the most probable set of hidden states given \n",
    "        observation sequence x using the Viterbi algorithm\n",
    "\n",
    "        x can have dimension N x T\n",
    "        '''\n",
    "        return viterbi(x, self.pi, self.A, self.B)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "# the random state is fixed so that the initial state is identical.\n",
    "thmm = TensorizedHMM(2)\n",
    "hmm = HMM(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dict_tensor.pickle\", 'rb') as ff:\n",
    "    tensor_time_stats = pickle.load(ff)\n",
    "with open(\"./dict_original.pickle\", 'rb') as ff:\n",
    "    basic_time_stats = pickle.load(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_time_stats = dict()\n",
    "for m_iter in [10,30,50,100]:\n",
    "    tensor_time_stats[m_iter] = list()\n",
    "    for exp_ in range(500):\n",
    "        #if exp_ % 10 == 0:\n",
    "        #    print(exp_)\n",
    "        start = time.time()\n",
    "        thmm = TensorizedHMM(2)\n",
    "        thmm.fit(X_data, max_iter=m_iter)\n",
    "        tensor_time_stats[m_iter].append(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_241/3611249952.py:59: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  self.pi = np.sum((alphas[n][0] * betas[n][0])/P[n] for n in range(num_sequences))/num_sequences\n"
     ]
    }
   ],
   "source": [
    "basic_time_stats = dict()\n",
    "for m_iter in [10,30,50,100]:\n",
    "    basic_time_stats[m_iter] = list()\n",
    "    for exp_ in range(500):\n",
    "        #if exp_ % 10 == 0:\n",
    "        #    print(exp_)\n",
    "        start = time.time()\n",
    "        hmm = HMM(2)\n",
    "        hmm.fit(X_data, max_iter=m_iter)\n",
    "        basic_time_stats[m_iter].append(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(tensor_time_stats[30])\n",
    "plt.hist(basic_time_stats[30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(tensor_time_stats[50])\n",
    "plt.hist(basic_time_stats[50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(tensor_time_stats[10])\n",
    "plt.hist(basic_time_stats[10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(tensor_time_stats[100])\n",
    "plt.hist(basic_time_stats[100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09674882888793945\n",
      "0.2788271903991699\n"
     ]
    }
   ],
   "source": [
    "print(np.max(tensor_time_stats[10]))\n",
    "print(np.max(basic_time_stats[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47867298126220703\n",
      "2.8119468688964844\n"
     ]
    }
   ],
   "source": [
    "print(np.max(tensor_time_stats[100]))\n",
    "print(np.max(basic_time_stats[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22649335861206055\n",
      "1.3389456272125244\n"
     ]
    }
   ],
   "source": [
    "print(np.max(tensor_time_stats[30]))\n",
    "print(np.max(basic_time_stats[30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17881441116333008\n",
      "1.656498908996582\n"
     ]
    }
   ],
   "source": [
    "print(np.max(tensor_time_stats[50]))\n",
    "print(np.max(basic_time_stats[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"with open(\"./dict_tensor.pickle\", 'wb') as ff:\n",
    "    pickle.dump(tensor_time_stats, ff)\n",
    "with open(\"./dict_original.pickle\", 'wb') as ff:\n",
    "    pickle.dump(basic_time_stats, ff)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile, pstats, io\n",
    "from pstats import SortKey\n",
    "\n",
    "def profile(fnc):\n",
    "    '''A decorator that uses CProfile for profiling a function fnc'''\n",
    "    \n",
    "    def inner(*args, **kwargs):\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "        retval = fnc(*args, **kwargs)\n",
    "        pr.disable()\n",
    "        s = io.StringIO()\n",
    "        sortby = SortKey.CUMULATIVE\n",
    "        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "        ps.print_stats()\n",
    "        print(s.getvalue())\n",
    "        return retval\n",
    "    \n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19f2dcfaf593399b30a8f53af9c897705b95d8f5016d0f862fa64694b4ff6a68"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('HMMs_udemy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

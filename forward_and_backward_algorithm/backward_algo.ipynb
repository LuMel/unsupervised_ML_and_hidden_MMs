{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward algorithm\n",
    "\n",
    "(It is recommended to first go through the forward-algorithm notebook to check out all the notation and the logic behind these algortihms)\n",
    "\n",
    "This is a recursive algorithm that, as the forward algorithm, serves to compute $$ p(x_1,...,x_T)$$ in polynomial time in $T$.\n",
    "\n",
    "It uses the quantity $\\beta(t,i)$ in a backward-recursive way, with initial value $$ \\beta(t=T, i)=1\\,, \\forall i$$ Note that the \"initial\" value occurs at $t=T$ in this algorithm. The backward-recursive algorithm reads $$ \\beta(t, i) = \\sum_j^M A(i,j)B(j, x_{t+1})\\beta(t+1, j)$$\n",
    "\n",
    "This makes $\\beta$ equal to $$ \\beta(t,i) = p(x_{t+1},...,x_T| z_t = i) $$ and $$ p(x_1,...,x_T) = \\sum_{z_1=1}^M \\pi(z_1) B(z_1, x_1) \\beta(t=1, z_1)$$\n",
    "\n",
    "Recall that the expression for $p(x_1,...,x_T)$ can be written in terms of transition matrices $A_{ij} = p(z_{t+1}=j| z_t =i), B_{jk} = p(x_t=k| z_t = j)$ and initial probabilities $\\pi_j = p(z_1=j)$: $$ p(x_1,...,x_T) = \\sum_{z_1}^M ...\\sum_{z_T}^M \\pi_{z_1} \\prod_t^T A_{z_{t+1}, z_t} B_{z_t, x_t}$$ but this explicit computation scales exponentially with $T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.array([0.1, 0.1, 0.8]).reshape(3,1) # 3x1\n",
    "B = np.array([[0.1, 0.9],[0.35, 0.65],[0., 1.]]) # 3x2\n",
    "A = np.array([[0.25, 0.25, 0.5], [0.4, 0.2, 0.4], [0.75, 0.2, 0.05]]) #3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume there exist matrices A and B and also vector \\pi\n",
    "# dim(A) = M x M, dim(B) = M x K, and dim(\\pi) = M x 1\n",
    "# for p(x1, ..., xT), the xs are fixed. This boilds down to picking a column of the resulting matrix\n",
    "def beta_initial(A_dim: int) -> np.ndarray:\n",
    "    '''\n",
    "    Computes the value of beta associated to initial time t=T\n",
    "\n",
    "    Parameters\n",
    "        A_dim: int\n",
    "            dimension of matrix A (namely, M in the above notation)\n",
    "\n",
    "    Return\n",
    "        np.ndarray: initial value of beta (namely, beta_1)\n",
    "    '''\n",
    "    return np.ones(shape=(A_dim, 1)) # column vector of ones (one component per each z-state)\n",
    "\n",
    "def beta_recurrent(\n",
    "                    A: Union[List, np.ndarray], \n",
    "                    B: Union[List, np.ndarray], \n",
    "                    sequence_length: int, \n",
    "                    beta_ini: np.ndarray\n",
    "                    ) -> List[np.ndarray]:\n",
    "    '''\n",
    "    Computes a set of alphas following the (recursive) forward algorithm\n",
    "\n",
    "    Parameters\n",
    "        A:  Union[List, np.ndarray]\n",
    "            Initial vector of probabilities (i.e. p(z_1)) for each possible state of z_1\n",
    "        B:  Union[List, np.ndarray]\n",
    "            time-independent matrix of transition probabiltiies p(x_t|z_t) for each x- and z-states\n",
    "        sequence_length: int\n",
    "            The length of the input sequence of xs whose join probability we want to compute\n",
    "        alpha_ini: np,ndarray\n",
    "            The value of alpha(t=1, z_1)\n",
    "    Return\n",
    "        List[np.ndarray]: list of alphas of the form \n",
    "        [alpha(t=1, z_1),..., alpha(t=T-1, z_{T-1}), alpha(t=T, z_T)]\n",
    "        (note that each alpha will have a different dimensionality as a tensor: (M, K,...,K))\n",
    "    '''\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    assert isinstance(sequence_length, int), 'the length of the sequence must be an integer number'\n",
    "    assert beta_ini.shape[0] == A.shape[0], 'the length of the first axis of beta_ini must be the rows of A'\n",
    "\n",
    "    # initialize betas list\n",
    "    betas_list = [0]*sequence_length\n",
    "    betas_list[-1] = beta_ini \n",
    "    for t_ in reversed(range(1,sequence_length)):\n",
    "        # elipsis ... stand for \"any untouched extra dimension\"\n",
    "        # note that we don't use t+1 when indexing beta because \n",
    "        # python starts counting indexes from 0: reversed(range(0,sequence_length))\n",
    "        # goes from sequence_length-1 to 0. The last index of betas_list\n",
    "        # is also sequence_length-1\n",
    "        betas_list[t_-1] = np.einsum('ij,js,j...->is...', A, B, betas_list[t_])\n",
    "        # note that index associated to z is first index of beta\n",
    "        # (this is so also for beta_ini)\n",
    "    return betas_list\n",
    "\n",
    "def get_general_probability_from_beta(\n",
    "                                      beta: np.ndarray, \n",
    "                                      pi: np.ndarray, \n",
    "                                      B:np.ndarray\n",
    "                                      ) -> np.ndarray:\n",
    "    '''\n",
    "    Computes the probability of a generic sequence of x states given a beta \n",
    "    (this beta is suppossed to correspond to beta(t=1, z_1))\n",
    "\n",
    "    Parameters:\n",
    "        beta: np.ndarray\n",
    "            A tensor with dimensions (M, K, K,..., K, 1)\n",
    "        pi: np.ndarray\n",
    "            A vector with dimension Mx1\n",
    "        B: np.ndarray\n",
    "            A matrix with dimension MxK\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            A tensor with dimension (K, K,...,K)\n",
    "    '''\n",
    "    # compute sum_z1 pi(z1) B(z1,x1) beta(t=1,z1)\n",
    "    # returns probability for arbitrary set of x\n",
    "    # (note the ordering: ijk... will correspond to x_1, x_2, ..., x_T)\n",
    "    # also, the fact that we defined beta_ini to be Mx1 and\n",
    "    # dim(pi)=Mx1 as well implies that after np.einsum we will get\n",
    "    # a tensor of dim 1 x K x K x ... x 1\n",
    "    # remove the first and last dummy dimensions s, l\n",
    "    # by explicitly \"summing\" over them\n",
    "    return np.einsum('is,ij,i...ml->j...m', pi, B, beta)\n",
    "\n",
    "def get_probability_sequence(\n",
    "                            pi: Union[List, np.ndarray], \n",
    "                            A: Union[List, np.ndarray], \n",
    "                            B: Union[List, np.ndarray], \n",
    "                            xs_sequence: Union[List, np.ndarray]\n",
    "                            ) -> float:\n",
    "    ''' \n",
    "    Computes the probability of a specific sequence of x states\n",
    "\n",
    "    Parameters:\n",
    "        pi: Union[List, np.ndarray]\n",
    "            Initial vector of probabilities (i.e. p(z_1)) for each possible state of z_1\n",
    "        A:  Union[List, np.ndarray]\n",
    "            Initial vector of probabilities (i.e. p(z_1)) for each possible state of z_1\n",
    "        B:  Union[List, np.ndarray]\n",
    "            time-independent matrix of transition probabiltiies p(x_t|z_t) for each x- and z-states\n",
    "        xs_sequence: Union[List, np.ndarray]\n",
    "            Full list of x-states for which to compute the probability.\n",
    "            The order must be [x_T, x_{T-1},..., x_1]\n",
    "\n",
    "    Returns:\n",
    "        float: The probability of the input sequence.\n",
    "    '''\n",
    "    # xs_sequence must be ordered as [x_1, x_2,..., x_T]\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    xs_sequence = np.array(xs_sequence)\n",
    "\n",
    "    assert A.shape[0] == A.shape[1], 'A must be a square matrix'\n",
    "    assert A.shape[0] == B.shape[0], 'number of rows of A and B must be the same'\n",
    "    \n",
    "    pi = np.array(pi).reshape(pi.size, 1)\n",
    "    assert pi.shape[0] == B.shape[0], 'number of rows of pi and B must be the same'\n",
    "    # x_t states can go from 1 to K\n",
    "    assert (xs_sequence <= B.shape[1]).all(), 'there are states in input sequence that go beyond dimensions of B'\n",
    "    \n",
    "    T_ = len(xs_sequence)\n",
    "    M_ = B.shape[0]\n",
    "    K_ = B.shape[1]\n",
    "\n",
    "    # compute initial state\n",
    "    beta_ini = beta_initial(A.shape[0])\n",
    "    # get sequence of betas for arbitrary xs\n",
    "    beta_list = beta_recurrent(A, B, T_, beta_ini)\n",
    "    # get general probability for first beta from list\n",
    "    # dim(general_prob) = (K,K,...K)\n",
    "    general_prob = get_general_probability_from_beta(beta_list[0], pi, B)\n",
    "    # indices of general_prob start at 0 but x states are indexed by 1,..K\n",
    "    # thats why we use xs_sequence-1. \n",
    "    # pick x-index for each K-dimension\n",
    "    return general_prob[tuple(xs_sequence-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_l = beta_recurrent(A, B, 2, beta_initial(A.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_l[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.004975, 0.040025],\n",
       "       [0.133275, 0.821725]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_general_probability_from_beta(beta_l[0], pi, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012056062545468752"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability_sequence(pi, A, B, list(reversed([1,2,1,1,2,2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above number coincides with the result that we get by applying the forward algorithm (see notebook on forward algorithm). Note that the input list of xs has to be reversed with respect to the one that we input in the forward algorithm, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19f2dcfaf593399b30a8f53af9c897705b95d8f5016d0f862fa64694b4ff6a68"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('HMMs_udemy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
